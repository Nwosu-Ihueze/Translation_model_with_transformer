{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9JJ7FBw84tG"
   },
   "source": [
    "# Stage 1: Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xL5ITr3SSMs8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "shAM-IpfSQl1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQN8jwx48_yU"
   },
   "source": [
    "# Stage 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPlOT-2mlw0r"
   },
   "source": [
    "## Loading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCD9jwXsLwS_"
   },
   "source": [
    "We import files from our personal google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PzkrgOZESc58",
    "outputId": "67b931d3-9204-496e-bced-06b149ed0b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zyPE07JOU8HW"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/My Drive/Transformer_eng_to_deu/de-en/europarl-v7.de-en.en\",\n",
    "          mode='r',\n",
    "          encoding='utf-8') as f:\n",
    "    europarl_en = f.read()\n",
    "with open(\"/content/drive/My Drive/Transformer_eng_to_deu/de-en/europarl-v7.de-en.de\",\n",
    "          mode='r',\n",
    "          encoding='utf-8') as f:\n",
    "    europarl_de = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgnbqkIuV51K",
    "outputId": "147fa9a7-f232-4638-e40a-4eafc676e6be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiederaufnahme der Sitzungsperiode\n",
      "Ich erklÃ¤re die\n"
     ]
    }
   ],
   "source": [
    "print(europarl_de[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEFw0D2vP_Dl"
   },
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwIBeGXn7LIJ"
   },
   "source": [
    "Getting the non_breaking_prefixes as a clean list of words with a point at the end so it is easier to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9x4mZfKMaxD"
   },
   "source": [
    "We will need each word and other symbol that we want to keep to be in lower case and separated by spaces so we can \"tokenize\" them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LhmvKegAWevF"
   },
   "outputs": [],
   "source": [
    "corpus_en = europarl_en\n",
    "\n",
    "\n",
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
    "\n",
    "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
    "\n",
    "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')\n",
    "\n",
    "corpus_de = europarl_de\n",
    "\n",
    "corpus_de = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_de)\n",
    "corpus_de = re.sub(r\".\\$\\$\\$\", '', corpus_de)\n",
    "corpus_de = re.sub(r\"  +\", \" \", corpus_de)\n",
    "corpus_de = corpus_de.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-Y9v8-Tozl2"
   },
   "source": [
    "## Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "357ud8xcXAFo"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_en, target_vocab_size=2**13)\n",
    "tokenizer_de = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_de, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fvmVMGwnX5Zq"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8190\n",
    "VOCAB_SIZE_DE = tokenizer_de.vocab_size + 2 # = 8171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6TaW_3U4X6nV"
   },
   "outputs": [],
   "source": [
    "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
    "          for sentence in corpus_en]\n",
    "outputs = [[VOCAB_SIZE_DE-2] + tokenizer_de.encode(sentence) + [VOCAB_SIZE_DE-1]\n",
    "           for sentence in corpus_de]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bG6AlcFMpC5C"
   },
   "source": [
    "## Remove too long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kRMnMjfjYLDi"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypm8h5aZQTZ1"
   },
   "source": [
    "## Inputs/outputs creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yMrBz12TYQYA"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TJ-6jFCLYYN0"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FP0WPsdM8hl"
   },
   "source": [
    "As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycT0YqydRcUd"
   },
   "source": [
    "# Stage 3: Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SBoH8G4XyR9"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G9C3ucmJ86I"
   },
   "source": [
    "Positional encoding formulae:\n",
    "\n",
    "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
    "\n",
    "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Mk5qouMnYhtp"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
    "        return pos * angles\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcw8YIQqRhOJ"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sffhwwvX-wj"
   },
   "source": [
    "### Attention computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VBuW6lESLDX"
   },
   "source": [
    "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "D_DhVJSZYp3o"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    \n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    \n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MjtvXrfYEx7"
   },
   "source": [
    "### Multi-head attention sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SInz93BWYvpv"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, nb_proj):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_proj = nb_proj\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_proj == 0\n",
    "        \n",
    "        self.d_proj = self.d_model // self.nb_proj\n",
    "        \n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.nb_proj,\n",
    "                 self.d_proj)\n",
    "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
    "    \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        \n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "        \n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "        \n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiyuHe1OeT5N"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "sbtkaICJY4ka"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, mask, training):\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                              inputs,\n",
    "                                              inputs,\n",
    "                                              mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "cY6ZWErcZCgv"
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.enc_layers = [EncoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DthraBEwuvl"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sLsXcra4Y6ad"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        # Self multi head attention\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Multi head attention combined with encoder output\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed foward\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
    "                                    activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        attention = self.multi_head_attention_1(inputs,\n",
    "                                                inputs,\n",
    "                                                inputs,\n",
    "                                                mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        attention_2 = self.multi_head_attention_2(attention,\n",
    "                                                  enc_outputs,\n",
    "                                                  enc_outputs,\n",
    "                                                  mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "        \n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "haIigqHUY-t8"
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for i in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.dec_layers[i](outputs,\n",
    "                                         enc_outputs,\n",
    "                                         mask_1,\n",
    "                                         mask_2,\n",
    "                                         training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5sJYkjbz5DD"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rIXxxrlfZNuj"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "        \n",
    "        self.encoder = Encoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_enc,\n",
    "                               d_model)\n",
    "        self.decoder = Decoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_dec,\n",
    "                               d_model)\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
    "    \n",
    "    def create_padding_mask(self, seq):\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "    \n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(\n",
    "            self.create_padding_mask(dec_inputs),\n",
    "            self.create_look_ahead_mask(dec_inputs)\n",
    "        )\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "        \n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        dec_outputs = self.decoder(dec_inputs,\n",
    "                                   enc_outputs,\n",
    "                                   dec_mask_1,\n",
    "                                   dec_mask_2,\n",
    "                                   training)\n",
    "        \n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c-LRThUPrso"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DnZfFS3wZZEV"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "D_MODEL = 128 # 512\n",
    "NB_LAYERS = 4 # 6\n",
    "FFN_UNITS = 512 # 2048\n",
    "NB_PROJ = 8 # 8\n",
    "DROPOUT_RATE = 0.1 # 0.1\n",
    "\n",
    "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_DE,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Sod7sfmrZZ6A"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=\"none\")\n",
    "\n",
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "MI4l-4epZdCw"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "leaning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7NloHGxaZgif"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./drive/My Drive/projects/transformer/ckpt/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Q2mN6xvZjhq",
    "outputId": "1e04fc71-cea4-4191-9f44-bcfed1feeddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Batch 0 Loss 6.2678 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 6.0136 Accuracy 0.0031\n",
      "Epoch 1 Batch 100 Loss 5.9185 Accuracy 0.0270\n",
      "Epoch 1 Batch 150 Loss 5.9033 Accuracy 0.0355\n",
      "Epoch 1 Batch 200 Loss 5.8258 Accuracy 0.0397\n",
      "Epoch 1 Batch 250 Loss 5.7447 Accuracy 0.0423\n",
      "Epoch 1 Batch 300 Loss 5.6427 Accuracy 0.0474\n",
      "Epoch 1 Batch 350 Loss 5.5371 Accuracy 0.0536\n",
      "Epoch 1 Batch 400 Loss 5.4325 Accuracy 0.0586\n",
      "Epoch 1 Batch 450 Loss 5.3327 Accuracy 0.0625\n",
      "Epoch 1 Batch 500 Loss 5.2403 Accuracy 0.0660\n",
      "Epoch 1 Batch 550 Loss 5.1542 Accuracy 0.0697\n",
      "Epoch 1 Batch 600 Loss 5.0789 Accuracy 0.0735\n",
      "Epoch 1 Batch 650 Loss 4.9978 Accuracy 0.0771\n",
      "Epoch 1 Batch 700 Loss 4.9264 Accuracy 0.0808\n",
      "Epoch 1 Batch 750 Loss 4.8532 Accuracy 0.0846\n",
      "Epoch 1 Batch 800 Loss 4.7875 Accuracy 0.0884\n",
      "Epoch 1 Batch 850 Loss 4.7230 Accuracy 0.0921\n",
      "Epoch 1 Batch 900 Loss 4.6657 Accuracy 0.0959\n",
      "Epoch 1 Batch 950 Loss 4.6089 Accuracy 0.0997\n",
      "Epoch 1 Batch 1000 Loss 4.5544 Accuracy 0.1033\n",
      "Epoch 1 Batch 1050 Loss 4.5003 Accuracy 0.1067\n",
      "Epoch 1 Batch 1100 Loss 4.4486 Accuracy 0.1099\n",
      "Epoch 1 Batch 1150 Loss 4.4017 Accuracy 0.1130\n",
      "Epoch 1 Batch 1200 Loss 4.3568 Accuracy 0.1160\n",
      "Epoch 1 Batch 1250 Loss 4.3140 Accuracy 0.1188\n",
      "Epoch 1 Batch 1300 Loss 4.2747 Accuracy 0.1215\n",
      "Epoch 1 Batch 1350 Loss 4.2350 Accuracy 0.1241\n",
      "Epoch 1 Batch 1400 Loss 4.1992 Accuracy 0.1266\n",
      "Epoch 1 Batch 1450 Loss 4.1640 Accuracy 0.1291\n",
      "Epoch 1 Batch 1500 Loss 4.1283 Accuracy 0.1314\n",
      "Epoch 1 Batch 1550 Loss 4.0941 Accuracy 0.1337\n",
      "Epoch 1 Batch 1600 Loss 4.0623 Accuracy 0.1359\n",
      "Epoch 1 Batch 1650 Loss 4.0308 Accuracy 0.1381\n",
      "Epoch 1 Batch 1700 Loss 4.0003 Accuracy 0.1401\n",
      "Epoch 1 Batch 1750 Loss 3.9725 Accuracy 0.1422\n",
      "Epoch 1 Batch 1800 Loss 3.9445 Accuracy 0.1441\n",
      "Epoch 1 Batch 1850 Loss 3.9179 Accuracy 0.1460\n",
      "Epoch 1 Batch 1900 Loss 3.8908 Accuracy 0.1477\n",
      "Epoch 1 Batch 1950 Loss 3.8639 Accuracy 0.1494\n",
      "Epoch 1 Batch 2000 Loss 3.8374 Accuracy 0.1510\n",
      "Epoch 1 Batch 2050 Loss 3.8122 Accuracy 0.1527\n",
      "Epoch 1 Batch 2100 Loss 3.7876 Accuracy 0.1543\n",
      "Epoch 1 Batch 2150 Loss 3.7643 Accuracy 0.1559\n",
      "Epoch 1 Batch 2200 Loss 3.7413 Accuracy 0.1574\n",
      "Epoch 1 Batch 2250 Loss 3.7183 Accuracy 0.1589\n",
      "Epoch 1 Batch 2300 Loss 3.6965 Accuracy 0.1603\n",
      "Epoch 1 Batch 2350 Loss 3.6751 Accuracy 0.1617\n",
      "Epoch 1 Batch 2400 Loss 3.6559 Accuracy 0.1631\n",
      "Epoch 1 Batch 2450 Loss 3.6358 Accuracy 0.1644\n",
      "Epoch 1 Batch 2500 Loss 3.6151 Accuracy 0.1658\n",
      "Epoch 1 Batch 2550 Loss 3.5959 Accuracy 0.1672\n",
      "Epoch 1 Batch 2600 Loss 3.5756 Accuracy 0.1686\n",
      "Epoch 1 Batch 2650 Loss 3.5571 Accuracy 0.1699\n",
      "Epoch 1 Batch 2700 Loss 3.5384 Accuracy 0.1714\n",
      "Epoch 1 Batch 2750 Loss 3.5218 Accuracy 0.1727\n",
      "Epoch 1 Batch 2800 Loss 3.5050 Accuracy 0.1741\n",
      "Epoch 1 Batch 2850 Loss 3.4879 Accuracy 0.1755\n",
      "Epoch 1 Batch 2900 Loss 3.4727 Accuracy 0.1769\n",
      "Epoch 1 Batch 2950 Loss 3.4573 Accuracy 0.1782\n",
      "Epoch 1 Batch 3000 Loss 3.4419 Accuracy 0.1795\n",
      "Epoch 1 Batch 3050 Loss 3.4271 Accuracy 0.1808\n",
      "Epoch 1 Batch 3100 Loss 3.4125 Accuracy 0.1820\n",
      "Epoch 1 Batch 3150 Loss 3.3978 Accuracy 0.1834\n",
      "Epoch 1 Batch 3200 Loss 3.3834 Accuracy 0.1846\n",
      "Epoch 1 Batch 3250 Loss 3.3693 Accuracy 0.1859\n",
      "Epoch 1 Batch 3300 Loss 3.3563 Accuracy 0.1871\n",
      "Epoch 1 Batch 3350 Loss 3.3438 Accuracy 0.1882\n",
      "Epoch 1 Batch 3400 Loss 3.3317 Accuracy 0.1894\n",
      "Epoch 1 Batch 3450 Loss 3.3190 Accuracy 0.1905\n",
      "Epoch 1 Batch 3500 Loss 3.3074 Accuracy 0.1917\n",
      "Epoch 1 Batch 3550 Loss 3.2957 Accuracy 0.1928\n",
      "Epoch 1 Batch 3600 Loss 3.2845 Accuracy 0.1939\n",
      "Epoch 1 Batch 3650 Loss 3.2724 Accuracy 0.1950\n",
      "Epoch 1 Batch 3700 Loss 3.2608 Accuracy 0.1960\n",
      "Epoch 1 Batch 3750 Loss 3.2492 Accuracy 0.1971\n",
      "Epoch 1 Batch 3800 Loss 3.2382 Accuracy 0.1982\n",
      "Epoch 1 Batch 3850 Loss 3.2277 Accuracy 0.1992\n",
      "Epoch 1 Batch 3900 Loss 3.2172 Accuracy 0.2003\n",
      "Epoch 1 Batch 3950 Loss 3.2069 Accuracy 0.2014\n",
      "Epoch 1 Batch 4000 Loss 3.1970 Accuracy 0.2025\n",
      "Epoch 1 Batch 4050 Loss 3.1870 Accuracy 0.2036\n",
      "Epoch 1 Batch 4100 Loss 3.1769 Accuracy 0.2047\n",
      "Epoch 1 Batch 4150 Loss 3.1667 Accuracy 0.2057\n",
      "Epoch 1 Batch 4200 Loss 3.1570 Accuracy 0.2068\n",
      "Epoch 1 Batch 4250 Loss 3.1470 Accuracy 0.2079\n",
      "Epoch 1 Batch 4300 Loss 3.1369 Accuracy 0.2090\n",
      "Epoch 1 Batch 4350 Loss 3.1267 Accuracy 0.2102\n",
      "Epoch 1 Batch 4400 Loss 3.1173 Accuracy 0.2113\n",
      "Epoch 1 Batch 4450 Loss 3.1080 Accuracy 0.2123\n",
      "Epoch 1 Batch 4500 Loss 3.0985 Accuracy 0.2133\n",
      "Epoch 1 Batch 4550 Loss 3.0897 Accuracy 0.2143\n",
      "Epoch 1 Batch 4600 Loss 3.0815 Accuracy 0.2151\n",
      "Epoch 1 Batch 4650 Loss 3.0731 Accuracy 0.2159\n",
      "Epoch 1 Batch 4700 Loss 3.0647 Accuracy 0.2167\n",
      "Epoch 1 Batch 4750 Loss 3.0567 Accuracy 0.2175\n",
      "Epoch 1 Batch 4800 Loss 3.0487 Accuracy 0.2182\n",
      "Epoch 1 Batch 4850 Loss 3.0402 Accuracy 0.2190\n",
      "Epoch 1 Batch 4900 Loss 3.0323 Accuracy 0.2197\n",
      "Epoch 1 Batch 4950 Loss 3.0247 Accuracy 0.2205\n",
      "Epoch 1 Batch 5000 Loss 3.0169 Accuracy 0.2212\n",
      "Epoch 1 Batch 5050 Loss 3.0090 Accuracy 0.2219\n",
      "Epoch 1 Batch 5100 Loss 3.0015 Accuracy 0.2226\n",
      "Epoch 1 Batch 5150 Loss 2.9940 Accuracy 0.2233\n",
      "Epoch 1 Batch 5200 Loss 2.9866 Accuracy 0.2240\n",
      "Epoch 1 Batch 5250 Loss 2.9791 Accuracy 0.2248\n",
      "Epoch 1 Batch 5300 Loss 2.9718 Accuracy 0.2255\n",
      "Epoch 1 Batch 5350 Loss 2.9642 Accuracy 0.2262\n",
      "Epoch 1 Batch 5400 Loss 2.9569 Accuracy 0.2269\n",
      "Epoch 1 Batch 5450 Loss 2.9497 Accuracy 0.2276\n",
      "Epoch 1 Batch 5500 Loss 2.9424 Accuracy 0.2283\n",
      "Epoch 1 Batch 5550 Loss 2.9350 Accuracy 0.2290\n",
      "Epoch 1 Batch 5600 Loss 2.9275 Accuracy 0.2296\n",
      "Epoch 1 Batch 5650 Loss 2.9207 Accuracy 0.2303\n",
      "Epoch 1 Batch 5700 Loss 2.9134 Accuracy 0.2309\n",
      "Epoch 1 Batch 5750 Loss 2.9060 Accuracy 0.2316\n",
      "Epoch 1 Batch 5800 Loss 2.8989 Accuracy 0.2322\n",
      "Epoch 1 Batch 5850 Loss 2.8920 Accuracy 0.2328\n",
      "Epoch 1 Batch 5900 Loss 2.8848 Accuracy 0.2334\n",
      "Epoch 1 Batch 5950 Loss 2.8777 Accuracy 0.2340\n",
      "Epoch 1 Batch 6000 Loss 2.8709 Accuracy 0.2346\n",
      "Epoch 1 Batch 6050 Loss 2.8638 Accuracy 0.2352\n",
      "Epoch 1 Batch 6100 Loss 2.8568 Accuracy 0.2359\n",
      "Epoch 1 Batch 6150 Loss 2.8497 Accuracy 0.2365\n",
      "Epoch 1 Batch 6200 Loss 2.8425 Accuracy 0.2371\n",
      "Saving checkpoint for epoch 1 at ./drive/My Drive/projects/transformer/ckpt/ckpt-1\n",
      "Time taken for 1 epoch: 1563.1175346374512 secs\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Batch 0 Loss 2.2595 Accuracy 0.2952\n",
      "Epoch 2 Batch 50 Loss 2.0954 Accuracy 0.3105\n",
      "Epoch 2 Batch 100 Loss 2.0680 Accuracy 0.3119\n",
      "Epoch 2 Batch 150 Loss 2.0556 Accuracy 0.3125\n",
      "Epoch 2 Batch 200 Loss 2.0486 Accuracy 0.3132\n",
      "Epoch 2 Batch 250 Loss 2.0493 Accuracy 0.3139\n",
      "Epoch 2 Batch 300 Loss 2.0351 Accuracy 0.3150\n",
      "Epoch 2 Batch 350 Loss 2.0323 Accuracy 0.3152\n",
      "Epoch 2 Batch 400 Loss 2.0273 Accuracy 0.3160\n",
      "Epoch 2 Batch 450 Loss 2.0230 Accuracy 0.3166\n",
      "Epoch 2 Batch 500 Loss 2.0158 Accuracy 0.3169\n",
      "Epoch 2 Batch 550 Loss 2.0078 Accuracy 0.3176\n",
      "Epoch 2 Batch 600 Loss 2.0021 Accuracy 0.3180\n",
      "Epoch 2 Batch 650 Loss 1.9989 Accuracy 0.3185\n",
      "Epoch 2 Batch 700 Loss 1.9928 Accuracy 0.3192\n",
      "Epoch 2 Batch 750 Loss 1.9901 Accuracy 0.3195\n",
      "Epoch 2 Batch 800 Loss 1.9849 Accuracy 0.3200\n",
      "Epoch 2 Batch 850 Loss 1.9829 Accuracy 0.3204\n",
      "Epoch 2 Batch 900 Loss 1.9811 Accuracy 0.3206\n",
      "Epoch 2 Batch 950 Loss 1.9799 Accuracy 0.3208\n",
      "Epoch 2 Batch 1000 Loss 1.9788 Accuracy 0.3211\n",
      "Epoch 2 Batch 1050 Loss 1.9762 Accuracy 0.3216\n",
      "Epoch 2 Batch 1100 Loss 1.9729 Accuracy 0.3218\n",
      "Epoch 2 Batch 1150 Loss 1.9682 Accuracy 0.3223\n",
      "Epoch 2 Batch 1200 Loss 1.9653 Accuracy 0.3224\n",
      "Epoch 2 Batch 1250 Loss 1.9610 Accuracy 0.3225\n",
      "Epoch 2 Batch 1300 Loss 1.9581 Accuracy 0.3229\n",
      "Epoch 2 Batch 1350 Loss 1.9543 Accuracy 0.3233\n",
      "Epoch 2 Batch 1400 Loss 1.9515 Accuracy 0.3236\n",
      "Epoch 2 Batch 1450 Loss 1.9475 Accuracy 0.3238\n",
      "Epoch 2 Batch 1500 Loss 1.9441 Accuracy 0.3242\n",
      "Epoch 2 Batch 1550 Loss 1.9419 Accuracy 0.3246\n",
      "Epoch 2 Batch 1600 Loss 1.9390 Accuracy 0.3250\n",
      "Epoch 2 Batch 1650 Loss 1.9359 Accuracy 0.3254\n",
      "Epoch 2 Batch 1700 Loss 1.9311 Accuracy 0.3257\n",
      "Epoch 2 Batch 1750 Loss 1.9274 Accuracy 0.3260\n",
      "Epoch 2 Batch 1800 Loss 1.9238 Accuracy 0.3263\n",
      "Epoch 2 Batch 1850 Loss 1.9194 Accuracy 0.3267\n",
      "Epoch 2 Batch 1900 Loss 1.9156 Accuracy 0.3271\n",
      "Epoch 2 Batch 1950 Loss 1.9109 Accuracy 0.3273\n",
      "Epoch 2 Batch 2000 Loss 1.9066 Accuracy 0.3276\n",
      "Epoch 2 Batch 2050 Loss 1.9025 Accuracy 0.3279\n",
      "Epoch 2 Batch 2100 Loss 1.8978 Accuracy 0.3281\n",
      "Epoch 2 Batch 2150 Loss 1.8939 Accuracy 0.3284\n",
      "Epoch 2 Batch 2200 Loss 1.8899 Accuracy 0.3287\n",
      "Epoch 2 Batch 2250 Loss 1.8856 Accuracy 0.3290\n",
      "Epoch 2 Batch 2300 Loss 1.8819 Accuracy 0.3292\n",
      "Epoch 2 Batch 2350 Loss 1.8769 Accuracy 0.3296\n",
      "Epoch 2 Batch 2400 Loss 1.8731 Accuracy 0.3300\n",
      "Epoch 2 Batch 2450 Loss 1.8690 Accuracy 0.3303\n",
      "Epoch 2 Batch 2500 Loss 1.8645 Accuracy 0.3308\n",
      "Epoch 2 Batch 2550 Loss 1.8598 Accuracy 0.3311\n",
      "Epoch 2 Batch 2600 Loss 1.8557 Accuracy 0.3316\n",
      "Epoch 2 Batch 2650 Loss 1.8510 Accuracy 0.3321\n",
      "Epoch 2 Batch 2700 Loss 1.8478 Accuracy 0.3325\n",
      "Epoch 2 Batch 2750 Loss 1.8439 Accuracy 0.3331\n",
      "Epoch 2 Batch 2800 Loss 1.8402 Accuracy 0.3336\n",
      "Epoch 2 Batch 2850 Loss 1.8365 Accuracy 0.3341\n",
      "Epoch 2 Batch 2900 Loss 1.8329 Accuracy 0.3348\n",
      "Epoch 2 Batch 2950 Loss 1.8292 Accuracy 0.3353\n",
      "Epoch 2 Batch 3000 Loss 1.8264 Accuracy 0.3359\n",
      "Epoch 2 Batch 3050 Loss 1.8232 Accuracy 0.3366\n",
      "Epoch 2 Batch 3100 Loss 1.8200 Accuracy 0.3372\n",
      "Epoch 2 Batch 3150 Loss 1.8167 Accuracy 0.3378\n",
      "Epoch 2 Batch 3200 Loss 1.8136 Accuracy 0.3384\n",
      "Epoch 2 Batch 3250 Loss 1.8106 Accuracy 0.3391\n",
      "Epoch 2 Batch 3300 Loss 1.8077 Accuracy 0.3396\n",
      "Epoch 2 Batch 3350 Loss 1.8048 Accuracy 0.3402\n",
      "Epoch 2 Batch 3400 Loss 1.8025 Accuracy 0.3408\n",
      "Epoch 2 Batch 3450 Loss 1.7998 Accuracy 0.3414\n",
      "Epoch 2 Batch 3500 Loss 1.7966 Accuracy 0.3420\n",
      "Epoch 2 Batch 3550 Loss 1.7934 Accuracy 0.3426\n",
      "Epoch 2 Batch 3600 Loss 1.7899 Accuracy 0.3432\n",
      "Epoch 2 Batch 3650 Loss 1.7865 Accuracy 0.3439\n",
      "Epoch 2 Batch 3700 Loss 1.7835 Accuracy 0.3445\n",
      "Epoch 2 Batch 3750 Loss 1.7806 Accuracy 0.3451\n",
      "Epoch 2 Batch 3800 Loss 1.7774 Accuracy 0.3457\n",
      "Epoch 2 Batch 3850 Loss 1.7747 Accuracy 0.3464\n",
      "Epoch 2 Batch 3900 Loss 1.7715 Accuracy 0.3471\n",
      "Epoch 2 Batch 3950 Loss 1.7687 Accuracy 0.3477\n",
      "Epoch 2 Batch 4000 Loss 1.7658 Accuracy 0.3484\n",
      "Epoch 2 Batch 4050 Loss 1.7630 Accuracy 0.3490\n",
      "Epoch 2 Batch 4100 Loss 1.7598 Accuracy 0.3496\n",
      "Epoch 2 Batch 4150 Loss 1.7571 Accuracy 0.3503\n",
      "Epoch 2 Batch 4200 Loss 1.7539 Accuracy 0.3510\n",
      "Epoch 2 Batch 4250 Loss 1.7509 Accuracy 0.3516\n",
      "Epoch 2 Batch 4300 Loss 1.7481 Accuracy 0.3523\n",
      "Epoch 2 Batch 4350 Loss 1.7452 Accuracy 0.3529\n",
      "Epoch 2 Batch 4400 Loss 1.7421 Accuracy 0.3536\n",
      "Epoch 2 Batch 4450 Loss 1.7395 Accuracy 0.3542\n",
      "Epoch 2 Batch 4500 Loss 1.7376 Accuracy 0.3547\n",
      "Epoch 2 Batch 4550 Loss 1.7360 Accuracy 0.3552\n",
      "Epoch 2 Batch 4600 Loss 1.7348 Accuracy 0.3555\n",
      "Epoch 2 Batch 4650 Loss 1.7339 Accuracy 0.3557\n",
      "Epoch 2 Batch 4700 Loss 1.7329 Accuracy 0.3559\n",
      "Epoch 2 Batch 4750 Loss 1.7319 Accuracy 0.3562\n",
      "Epoch 2 Batch 4800 Loss 1.7314 Accuracy 0.3563\n",
      "Epoch 2 Batch 4850 Loss 1.7309 Accuracy 0.3565\n",
      "Epoch 2 Batch 4900 Loss 1.7304 Accuracy 0.3566\n",
      "Epoch 2 Batch 4950 Loss 1.7298 Accuracy 0.3567\n",
      "Epoch 2 Batch 5000 Loss 1.7290 Accuracy 0.3569\n",
      "Epoch 2 Batch 5050 Loss 1.7279 Accuracy 0.3571\n",
      "Epoch 2 Batch 5100 Loss 1.7271 Accuracy 0.3572\n",
      "Epoch 2 Batch 5150 Loss 1.7267 Accuracy 0.3574\n",
      "Epoch 2 Batch 5200 Loss 1.7262 Accuracy 0.3576\n",
      "Epoch 2 Batch 5250 Loss 1.7251 Accuracy 0.3578\n",
      "Epoch 2 Batch 5300 Loss 1.7245 Accuracy 0.3579\n",
      "Epoch 2 Batch 5350 Loss 1.7241 Accuracy 0.3580\n",
      "Epoch 2 Batch 5400 Loss 1.7234 Accuracy 0.3582\n",
      "Epoch 2 Batch 5450 Loss 1.7227 Accuracy 0.3583\n",
      "Epoch 2 Batch 5500 Loss 1.7219 Accuracy 0.3584\n",
      "Epoch 2 Batch 5550 Loss 1.7211 Accuracy 0.3586\n",
      "Epoch 2 Batch 5600 Loss 1.7205 Accuracy 0.3586\n",
      "Epoch 2 Batch 5650 Loss 1.7199 Accuracy 0.3587\n",
      "Epoch 2 Batch 5700 Loss 1.7193 Accuracy 0.3587\n",
      "Epoch 2 Batch 5750 Loss 1.7184 Accuracy 0.3588\n",
      "Epoch 2 Batch 5800 Loss 1.7171 Accuracy 0.3589\n",
      "Epoch 2 Batch 5850 Loss 1.7159 Accuracy 0.3589\n",
      "Epoch 2 Batch 5900 Loss 1.7151 Accuracy 0.3590\n",
      "Epoch 2 Batch 5950 Loss 1.7141 Accuracy 0.3590\n",
      "Epoch 2 Batch 6000 Loss 1.7131 Accuracy 0.3591\n",
      "Epoch 2 Batch 6050 Loss 1.7122 Accuracy 0.3592\n",
      "Epoch 2 Batch 6100 Loss 1.7111 Accuracy 0.3593\n",
      "Epoch 2 Batch 6150 Loss 1.7101 Accuracy 0.3594\n",
      "Epoch 2 Batch 6200 Loss 1.7087 Accuracy 0.3595\n",
      "Saving checkpoint for epoch 2 at ./drive/My Drive/projects/transformer/ckpt/ckpt-2\n",
      "Time taken for 1 epoch: 1440.472023010254 secs\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch 3 Batch 0 Loss 1.4303 Accuracy 0.3627\n",
      "Epoch 3 Batch 50 Loss 1.6519 Accuracy 0.3671\n",
      "Epoch 3 Batch 100 Loss 1.6524 Accuracy 0.3690\n",
      "Epoch 3 Batch 150 Loss 1.6401 Accuracy 0.3695\n",
      "Epoch 3 Batch 200 Loss 1.6349 Accuracy 0.3708\n",
      "Epoch 3 Batch 250 Loss 1.6288 Accuracy 0.3708\n",
      "Epoch 3 Batch 300 Loss 1.6217 Accuracy 0.3713\n",
      "Epoch 3 Batch 350 Loss 1.6162 Accuracy 0.3717\n",
      "Epoch 3 Batch 400 Loss 1.6135 Accuracy 0.3723\n",
      "Epoch 3 Batch 450 Loss 1.6113 Accuracy 0.3730\n",
      "Epoch 3 Batch 500 Loss 1.6083 Accuracy 0.3731\n",
      "Epoch 3 Batch 550 Loss 1.6040 Accuracy 0.3735\n",
      "Epoch 3 Batch 600 Loss 1.6025 Accuracy 0.3734\n",
      "Epoch 3 Batch 650 Loss 1.6013 Accuracy 0.3734\n",
      "Epoch 3 Batch 700 Loss 1.5989 Accuracy 0.3733\n",
      "Epoch 3 Batch 750 Loss 1.5980 Accuracy 0.3730\n",
      "Epoch 3 Batch 800 Loss 1.5951 Accuracy 0.3733\n",
      "Epoch 3 Batch 850 Loss 1.5948 Accuracy 0.3735\n",
      "Epoch 3 Batch 900 Loss 1.5925 Accuracy 0.3738\n",
      "Epoch 3 Batch 950 Loss 1.5913 Accuracy 0.3739\n",
      "Epoch 3 Batch 1000 Loss 1.5901 Accuracy 0.3742\n",
      "Epoch 3 Batch 1050 Loss 1.5878 Accuracy 0.3743\n",
      "Epoch 3 Batch 1100 Loss 1.5878 Accuracy 0.3746\n",
      "Epoch 3 Batch 1150 Loss 1.5859 Accuracy 0.3747\n",
      "Epoch 3 Batch 1200 Loss 1.5827 Accuracy 0.3748\n",
      "Epoch 3 Batch 1250 Loss 1.5805 Accuracy 0.3748\n",
      "Epoch 3 Batch 1300 Loss 1.5780 Accuracy 0.3749\n",
      "Epoch 3 Batch 1350 Loss 1.5761 Accuracy 0.3750\n",
      "Epoch 3 Batch 1400 Loss 1.5747 Accuracy 0.3750\n",
      "Epoch 3 Batch 1450 Loss 1.5729 Accuracy 0.3752\n",
      "Epoch 3 Batch 1500 Loss 1.5705 Accuracy 0.3754\n",
      "Epoch 3 Batch 1550 Loss 1.5688 Accuracy 0.3758\n",
      "Epoch 3 Batch 1600 Loss 1.5675 Accuracy 0.3760\n",
      "Epoch 3 Batch 1650 Loss 1.5641 Accuracy 0.3763\n",
      "Epoch 3 Batch 1700 Loss 1.5616 Accuracy 0.3765\n",
      "Epoch 3 Batch 1750 Loss 1.5600 Accuracy 0.3767\n",
      "Epoch 3 Batch 1800 Loss 1.5572 Accuracy 0.3769\n",
      "Epoch 3 Batch 1850 Loss 1.5542 Accuracy 0.3771\n",
      "Epoch 3 Batch 1900 Loss 1.5519 Accuracy 0.3771\n",
      "Epoch 3 Batch 1950 Loss 1.5496 Accuracy 0.3771\n",
      "Epoch 3 Batch 2000 Loss 1.5466 Accuracy 0.3772\n",
      "Epoch 3 Batch 2050 Loss 1.5434 Accuracy 0.3774\n",
      "Epoch 3 Batch 2100 Loss 1.5407 Accuracy 0.3774\n",
      "Epoch 3 Batch 2150 Loss 1.5377 Accuracy 0.3775\n",
      "Epoch 3 Batch 2200 Loss 1.5352 Accuracy 0.3777\n",
      "Epoch 3 Batch 2250 Loss 1.5328 Accuracy 0.3778\n",
      "Epoch 3 Batch 2300 Loss 1.5303 Accuracy 0.3779\n",
      "Epoch 3 Batch 2350 Loss 1.5274 Accuracy 0.3781\n",
      "Epoch 3 Batch 2400 Loss 1.5250 Accuracy 0.3783\n",
      "Epoch 3 Batch 2450 Loss 1.5222 Accuracy 0.3785\n",
      "Epoch 3 Batch 2500 Loss 1.5184 Accuracy 0.3788\n",
      "Epoch 3 Batch 2550 Loss 1.5153 Accuracy 0.3790\n",
      "Epoch 3 Batch 2600 Loss 1.5123 Accuracy 0.3794\n",
      "Epoch 3 Batch 2650 Loss 1.5095 Accuracy 0.3797\n",
      "Epoch 3 Batch 2700 Loss 1.5070 Accuracy 0.3800\n",
      "Epoch 3 Batch 2750 Loss 1.5044 Accuracy 0.3803\n",
      "Epoch 3 Batch 2800 Loss 1.5023 Accuracy 0.3807\n",
      "Epoch 3 Batch 2850 Loss 1.5001 Accuracy 0.3810\n",
      "Epoch 3 Batch 2900 Loss 1.4980 Accuracy 0.3815\n",
      "Epoch 3 Batch 2950 Loss 1.4958 Accuracy 0.3820\n",
      "Epoch 3 Batch 3000 Loss 1.4932 Accuracy 0.3825\n",
      "Epoch 3 Batch 3050 Loss 1.4910 Accuracy 0.3830\n",
      "Epoch 3 Batch 3100 Loss 1.4891 Accuracy 0.3833\n",
      "Epoch 3 Batch 3150 Loss 1.4868 Accuracy 0.3837\n",
      "Epoch 3 Batch 3200 Loss 1.4849 Accuracy 0.3842\n",
      "Epoch 3 Batch 3250 Loss 1.4834 Accuracy 0.3847\n",
      "Epoch 3 Batch 3300 Loss 1.4815 Accuracy 0.3851\n",
      "Epoch 3 Batch 3350 Loss 1.4796 Accuracy 0.3856\n",
      "Epoch 3 Batch 3400 Loss 1.4783 Accuracy 0.3861\n",
      "Epoch 3 Batch 3450 Loss 1.4765 Accuracy 0.3866\n",
      "Epoch 3 Batch 3500 Loss 1.4747 Accuracy 0.3870\n",
      "Epoch 3 Batch 3550 Loss 1.4729 Accuracy 0.3875\n",
      "Epoch 3 Batch 3600 Loss 1.4707 Accuracy 0.3880\n",
      "Epoch 3 Batch 3650 Loss 1.4685 Accuracy 0.3885\n",
      "Epoch 3 Batch 3700 Loss 1.4667 Accuracy 0.3889\n",
      "Epoch 3 Batch 3750 Loss 1.4650 Accuracy 0.3894\n",
      "Epoch 3 Batch 3800 Loss 1.4631 Accuracy 0.3899\n",
      "Epoch 3 Batch 3850 Loss 1.4605 Accuracy 0.3904\n",
      "Epoch 3 Batch 3900 Loss 1.4584 Accuracy 0.3910\n",
      "Epoch 3 Batch 3950 Loss 1.4566 Accuracy 0.3915\n",
      "Epoch 3 Batch 4000 Loss 1.4549 Accuracy 0.3920\n",
      "Epoch 3 Batch 4050 Loss 1.4531 Accuracy 0.3925\n",
      "Epoch 3 Batch 4100 Loss 1.4513 Accuracy 0.3931\n",
      "Epoch 3 Batch 4150 Loss 1.4495 Accuracy 0.3936\n",
      "Epoch 3 Batch 4200 Loss 1.4475 Accuracy 0.3942\n",
      "Epoch 3 Batch 4250 Loss 1.4456 Accuracy 0.3948\n",
      "Epoch 3 Batch 4300 Loss 1.4433 Accuracy 0.3953\n",
      "Epoch 3 Batch 4350 Loss 1.4415 Accuracy 0.3958\n",
      "Epoch 3 Batch 4400 Loss 1.4393 Accuracy 0.3964\n",
      "Epoch 3 Batch 4450 Loss 1.4380 Accuracy 0.3969\n",
      "Epoch 3 Batch 4500 Loss 1.4373 Accuracy 0.3972\n",
      "Epoch 3 Batch 4550 Loss 1.4370 Accuracy 0.3975\n",
      "Epoch 3 Batch 4600 Loss 1.4366 Accuracy 0.3977\n",
      "Epoch 3 Batch 4650 Loss 1.4368 Accuracy 0.3978\n",
      "Epoch 3 Batch 4700 Loss 1.4370 Accuracy 0.3978\n",
      "Epoch 3 Batch 4750 Loss 1.4377 Accuracy 0.3979\n",
      "Epoch 3 Batch 4800 Loss 1.4380 Accuracy 0.3979\n",
      "Epoch 3 Batch 4850 Loss 1.4386 Accuracy 0.3980\n",
      "Epoch 3 Batch 4900 Loss 1.4394 Accuracy 0.3980\n",
      "Epoch 3 Batch 4950 Loss 1.4394 Accuracy 0.3980\n",
      "Epoch 3 Batch 5000 Loss 1.4399 Accuracy 0.3980\n",
      "Epoch 3 Batch 5050 Loss 1.4404 Accuracy 0.3980\n",
      "Epoch 3 Batch 5100 Loss 1.4407 Accuracy 0.3980\n",
      "Epoch 3 Batch 5150 Loss 1.4410 Accuracy 0.3980\n",
      "Epoch 3 Batch 5200 Loss 1.4414 Accuracy 0.3980\n",
      "Epoch 3 Batch 5250 Loss 1.4419 Accuracy 0.3980\n",
      "Epoch 3 Batch 5300 Loss 1.4423 Accuracy 0.3980\n",
      "Epoch 3 Batch 5350 Loss 1.4425 Accuracy 0.3980\n",
      "Epoch 3 Batch 5400 Loss 1.4427 Accuracy 0.3980\n",
      "Epoch 3 Batch 5450 Loss 1.4431 Accuracy 0.3980\n",
      "Epoch 3 Batch 5500 Loss 1.4434 Accuracy 0.3980\n",
      "Epoch 3 Batch 5550 Loss 1.4440 Accuracy 0.3980\n",
      "Epoch 3 Batch 5600 Loss 1.4443 Accuracy 0.3979\n",
      "Epoch 3 Batch 5650 Loss 1.4445 Accuracy 0.3978\n",
      "Epoch 3 Batch 5700 Loss 1.4448 Accuracy 0.3978\n",
      "Epoch 3 Batch 5750 Loss 1.4449 Accuracy 0.3977\n",
      "Epoch 3 Batch 5800 Loss 1.4451 Accuracy 0.3976\n",
      "Epoch 3 Batch 5850 Loss 1.4452 Accuracy 0.3975\n",
      "Epoch 3 Batch 5900 Loss 1.4455 Accuracy 0.3974\n",
      "Epoch 3 Batch 5950 Loss 1.4452 Accuracy 0.3974\n",
      "Epoch 3 Batch 6000 Loss 1.4451 Accuracy 0.3973\n",
      "Epoch 3 Batch 6050 Loss 1.4448 Accuracy 0.3973\n",
      "Epoch 3 Batch 6100 Loss 1.4445 Accuracy 0.3972\n",
      "Epoch 3 Batch 6150 Loss 1.4446 Accuracy 0.3972\n",
      "Epoch 3 Batch 6200 Loss 1.4444 Accuracy 0.3971\n",
      "Saving checkpoint for epoch 3 at ./drive/My Drive/projects/transformer/ckpt/ckpt-3\n",
      "Time taken for 1 epoch: 1437.9058270454407 secs\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch 4 Batch 0 Loss 1.5524 Accuracy 0.4186\n",
      "Epoch 4 Batch 50 Loss 1.4941 Accuracy 0.3857\n",
      "Epoch 4 Batch 100 Loss 1.4864 Accuracy 0.3878\n",
      "Epoch 4 Batch 150 Loss 1.4846 Accuracy 0.3895\n",
      "Epoch 4 Batch 200 Loss 1.4813 Accuracy 0.3899\n",
      "Epoch 4 Batch 250 Loss 1.4745 Accuracy 0.3909\n",
      "Epoch 4 Batch 300 Loss 1.4704 Accuracy 0.3913\n",
      "Epoch 4 Batch 350 Loss 1.4692 Accuracy 0.3919\n",
      "Epoch 4 Batch 400 Loss 1.4671 Accuracy 0.3923\n",
      "Epoch 4 Batch 450 Loss 1.4634 Accuracy 0.3931\n",
      "Epoch 4 Batch 500 Loss 1.4610 Accuracy 0.3937\n",
      "Epoch 4 Batch 550 Loss 1.4592 Accuracy 0.3942\n",
      "Epoch 4 Batch 600 Loss 1.4556 Accuracy 0.3943\n",
      "Epoch 4 Batch 650 Loss 1.4545 Accuracy 0.3943\n",
      "Epoch 4 Batch 700 Loss 1.4546 Accuracy 0.3941\n",
      "Epoch 4 Batch 750 Loss 1.4538 Accuracy 0.3936\n",
      "Epoch 4 Batch 800 Loss 1.4520 Accuracy 0.3940\n",
      "Epoch 4 Batch 850 Loss 1.4500 Accuracy 0.3944\n",
      "Epoch 4 Batch 900 Loss 1.4476 Accuracy 0.3946\n",
      "Epoch 4 Batch 950 Loss 1.4470 Accuracy 0.3946\n",
      "Epoch 4 Batch 1000 Loss 1.4473 Accuracy 0.3945\n",
      "Epoch 4 Batch 1050 Loss 1.4451 Accuracy 0.3948\n",
      "Epoch 4 Batch 1100 Loss 1.4430 Accuracy 0.3951\n",
      "Epoch 4 Batch 1150 Loss 1.4411 Accuracy 0.3953\n",
      "Epoch 4 Batch 1200 Loss 1.4383 Accuracy 0.3952\n",
      "Epoch 4 Batch 1250 Loss 1.4377 Accuracy 0.3954\n",
      "Epoch 4 Batch 1300 Loss 1.4372 Accuracy 0.3955\n",
      "Epoch 4 Batch 1350 Loss 1.4362 Accuracy 0.3955\n",
      "Epoch 4 Batch 1400 Loss 1.4358 Accuracy 0.3957\n",
      "Epoch 4 Batch 1450 Loss 1.4335 Accuracy 0.3956\n",
      "Epoch 4 Batch 1500 Loss 1.4319 Accuracy 0.3957\n",
      "Epoch 4 Batch 1550 Loss 1.4305 Accuracy 0.3959\n",
      "Epoch 4 Batch 1600 Loss 1.4286 Accuracy 0.3961\n",
      "Epoch 4 Batch 1650 Loss 1.4265 Accuracy 0.3963\n",
      "Epoch 4 Batch 1700 Loss 1.4234 Accuracy 0.3966\n",
      "Epoch 4 Batch 1750 Loss 1.4214 Accuracy 0.3968\n",
      "Epoch 4 Batch 1800 Loss 1.4188 Accuracy 0.3969\n",
      "Epoch 4 Batch 1850 Loss 1.4164 Accuracy 0.3970\n",
      "Epoch 4 Batch 1900 Loss 1.4140 Accuracy 0.3972\n",
      "Epoch 4 Batch 1950 Loss 1.4118 Accuracy 0.3972\n",
      "Epoch 4 Batch 2000 Loss 1.4102 Accuracy 0.3973\n",
      "Epoch 4 Batch 2050 Loss 1.4079 Accuracy 0.3974\n",
      "Epoch 4 Batch 2100 Loss 1.4057 Accuracy 0.3974\n",
      "Epoch 4 Batch 2150 Loss 1.4038 Accuracy 0.3975\n",
      "Epoch 4 Batch 2200 Loss 1.4020 Accuracy 0.3975\n",
      "Epoch 4 Batch 2250 Loss 1.3996 Accuracy 0.3976\n",
      "Epoch 4 Batch 2300 Loss 1.3969 Accuracy 0.3977\n",
      "Epoch 4 Batch 2350 Loss 1.3944 Accuracy 0.3977\n",
      "Epoch 4 Batch 2400 Loss 1.3917 Accuracy 0.3979\n",
      "Epoch 4 Batch 2450 Loss 1.3899 Accuracy 0.3981\n",
      "Epoch 4 Batch 2500 Loss 1.3866 Accuracy 0.3982\n",
      "Epoch 4 Batch 2550 Loss 1.3834 Accuracy 0.3984\n",
      "Epoch 4 Batch 2600 Loss 1.3810 Accuracy 0.3987\n",
      "Epoch 4 Batch 2650 Loss 1.3782 Accuracy 0.3989\n",
      "Epoch 4 Batch 2700 Loss 1.3756 Accuracy 0.3992\n",
      "Epoch 4 Batch 2750 Loss 1.3731 Accuracy 0.3996\n",
      "Epoch 4 Batch 2800 Loss 1.3711 Accuracy 0.4000\n",
      "Epoch 4 Batch 2850 Loss 1.3692 Accuracy 0.4003\n",
      "Epoch 4 Batch 2900 Loss 1.3678 Accuracy 0.4007\n",
      "Epoch 4 Batch 2950 Loss 1.3659 Accuracy 0.4011\n",
      "Epoch 4 Batch 3000 Loss 1.3645 Accuracy 0.4014\n",
      "Epoch 4 Batch 3050 Loss 1.3627 Accuracy 0.4019\n",
      "Epoch 4 Batch 3100 Loss 1.3609 Accuracy 0.4023\n",
      "Epoch 4 Batch 3150 Loss 1.3593 Accuracy 0.4027\n",
      "Epoch 4 Batch 3200 Loss 1.3578 Accuracy 0.4030\n",
      "Epoch 4 Batch 3250 Loss 1.3565 Accuracy 0.4034\n",
      "Epoch 4 Batch 3300 Loss 1.3551 Accuracy 0.4039\n",
      "Epoch 4 Batch 3350 Loss 1.3537 Accuracy 0.4044\n",
      "Epoch 4 Batch 3400 Loss 1.3526 Accuracy 0.4048\n",
      "Epoch 4 Batch 3450 Loss 1.3510 Accuracy 0.4052\n",
      "Epoch 4 Batch 3500 Loss 1.3490 Accuracy 0.4057\n",
      "Epoch 4 Batch 3550 Loss 1.3474 Accuracy 0.4062\n",
      "Epoch 4 Batch 3600 Loss 1.3455 Accuracy 0.4066\n",
      "Epoch 4 Batch 3650 Loss 1.3439 Accuracy 0.4072\n",
      "Epoch 4 Batch 3700 Loss 1.3420 Accuracy 0.4077\n",
      "Epoch 4 Batch 3750 Loss 1.3403 Accuracy 0.4082\n",
      "Epoch 4 Batch 3800 Loss 1.3386 Accuracy 0.4086\n",
      "Epoch 4 Batch 3850 Loss 1.3369 Accuracy 0.4091\n",
      "Epoch 4 Batch 3900 Loss 1.3350 Accuracy 0.4097\n",
      "Epoch 4 Batch 3950 Loss 1.3334 Accuracy 0.4101\n",
      "Epoch 4 Batch 4000 Loss 1.3318 Accuracy 0.4106\n",
      "Epoch 4 Batch 4050 Loss 1.3301 Accuracy 0.4111\n",
      "Epoch 4 Batch 4100 Loss 1.3284 Accuracy 0.4116\n",
      "Epoch 4 Batch 4150 Loss 1.3264 Accuracy 0.4121\n",
      "Epoch 4 Batch 4200 Loss 1.3249 Accuracy 0.4126\n",
      "Epoch 4 Batch 4250 Loss 1.3230 Accuracy 0.4131\n",
      "Epoch 4 Batch 4300 Loss 1.3215 Accuracy 0.4137\n",
      "Epoch 4 Batch 4350 Loss 1.3197 Accuracy 0.4142\n",
      "Epoch 4 Batch 4400 Loss 1.3182 Accuracy 0.4147\n",
      "Epoch 4 Batch 4450 Loss 1.3170 Accuracy 0.4151\n",
      "Epoch 4 Batch 4500 Loss 1.3166 Accuracy 0.4154\n",
      "Epoch 4 Batch 4550 Loss 1.3165 Accuracy 0.4156\n",
      "Epoch 4 Batch 4600 Loss 1.3162 Accuracy 0.4157\n",
      "Epoch 4 Batch 4650 Loss 1.3168 Accuracy 0.4158\n",
      "Epoch 4 Batch 4700 Loss 1.3174 Accuracy 0.4158\n",
      "Epoch 4 Batch 4750 Loss 1.3180 Accuracy 0.4159\n",
      "Epoch 4 Batch 4800 Loss 1.3189 Accuracy 0.4159\n",
      "Epoch 4 Batch 4850 Loss 1.3196 Accuracy 0.4159\n",
      "Epoch 4 Batch 4900 Loss 1.3201 Accuracy 0.4158\n",
      "Epoch 4 Batch 4950 Loss 1.3208 Accuracy 0.4158\n",
      "Epoch 4 Batch 5000 Loss 1.3214 Accuracy 0.4158\n",
      "Epoch 4 Batch 5050 Loss 1.3220 Accuracy 0.4157\n",
      "Epoch 4 Batch 5100 Loss 1.3226 Accuracy 0.4156\n",
      "Epoch 4 Batch 5150 Loss 1.3234 Accuracy 0.4156\n",
      "Epoch 4 Batch 5200 Loss 1.3241 Accuracy 0.4156\n",
      "Epoch 4 Batch 5250 Loss 1.3246 Accuracy 0.4156\n",
      "Epoch 4 Batch 5300 Loss 1.3252 Accuracy 0.4155\n",
      "Epoch 4 Batch 5350 Loss 1.3259 Accuracy 0.4155\n",
      "Epoch 4 Batch 5400 Loss 1.3266 Accuracy 0.4154\n",
      "Epoch 4 Batch 5450 Loss 1.3274 Accuracy 0.4153\n",
      "Epoch 4 Batch 5500 Loss 1.3280 Accuracy 0.4153\n",
      "Epoch 4 Batch 5550 Loss 1.3288 Accuracy 0.4152\n",
      "Epoch 4 Batch 5600 Loss 1.3295 Accuracy 0.4151\n",
      "Epoch 4 Batch 5650 Loss 1.3300 Accuracy 0.4150\n",
      "Epoch 4 Batch 5700 Loss 1.3306 Accuracy 0.4149\n",
      "Epoch 4 Batch 5750 Loss 1.3311 Accuracy 0.4147\n",
      "Epoch 4 Batch 5800 Loss 1.3317 Accuracy 0.4146\n",
      "Epoch 4 Batch 5850 Loss 1.3321 Accuracy 0.4144\n",
      "Epoch 4 Batch 5900 Loss 1.3326 Accuracy 0.4143\n",
      "Epoch 4 Batch 5950 Loss 1.3328 Accuracy 0.4142\n",
      "Epoch 4 Batch 6000 Loss 1.3330 Accuracy 0.4141\n",
      "Epoch 4 Batch 6050 Loss 1.3329 Accuracy 0.4140\n",
      "Epoch 4 Batch 6100 Loss 1.3331 Accuracy 0.4140\n",
      "Epoch 4 Batch 6150 Loss 1.3331 Accuracy 0.4139\n",
      "Epoch 4 Batch 6200 Loss 1.3332 Accuracy 0.4138\n",
      "Saving checkpoint for epoch 4 at ./drive/My Drive/projects/transformer/ckpt/ckpt-4\n",
      "Time taken for 1 epoch: 1438.1015033721924 secs\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch 5 Batch 0 Loss 1.5498 Accuracy 0.4013\n",
      "Epoch 5 Batch 50 Loss 1.4138 Accuracy 0.4001\n",
      "Epoch 5 Batch 100 Loss 1.4047 Accuracy 0.4031\n",
      "Epoch 5 Batch 150 Loss 1.3935 Accuracy 0.4025\n",
      "Epoch 5 Batch 200 Loss 1.3919 Accuracy 0.4018\n",
      "Epoch 5 Batch 250 Loss 1.3907 Accuracy 0.4019\n",
      "Epoch 5 Batch 300 Loss 1.3892 Accuracy 0.4025\n",
      "Epoch 5 Batch 350 Loss 1.3917 Accuracy 0.4027\n",
      "Epoch 5 Batch 400 Loss 1.3885 Accuracy 0.4029\n",
      "Epoch 5 Batch 450 Loss 1.3839 Accuracy 0.4038\n",
      "Epoch 5 Batch 500 Loss 1.3810 Accuracy 0.4043\n",
      "Epoch 5 Batch 550 Loss 1.3803 Accuracy 0.4046\n",
      "Epoch 5 Batch 600 Loss 1.3803 Accuracy 0.4049\n",
      "Epoch 5 Batch 650 Loss 1.3779 Accuracy 0.4052\n",
      "Epoch 5 Batch 700 Loss 1.3753 Accuracy 0.4055\n",
      "Epoch 5 Batch 750 Loss 1.3737 Accuracy 0.4057\n",
      "Epoch 5 Batch 800 Loss 1.3737 Accuracy 0.4059\n",
      "Epoch 5 Batch 850 Loss 1.3724 Accuracy 0.4061\n",
      "Epoch 5 Batch 900 Loss 1.3716 Accuracy 0.4061\n",
      "Epoch 5 Batch 950 Loss 1.3715 Accuracy 0.4060\n",
      "Epoch 5 Batch 1000 Loss 1.3704 Accuracy 0.4062\n",
      "Epoch 5 Batch 1050 Loss 1.3695 Accuracy 0.4064\n",
      "Epoch 5 Batch 1100 Loss 1.3676 Accuracy 0.4065\n",
      "Epoch 5 Batch 1150 Loss 1.3662 Accuracy 0.4067\n",
      "Epoch 5 Batch 1200 Loss 1.3641 Accuracy 0.4065\n",
      "Epoch 5 Batch 1250 Loss 1.3615 Accuracy 0.4067\n",
      "Epoch 5 Batch 1300 Loss 1.3605 Accuracy 0.4068\n",
      "Epoch 5 Batch 1350 Loss 1.3593 Accuracy 0.4067\n",
      "Epoch 5 Batch 1400 Loss 1.3575 Accuracy 0.4068\n",
      "Epoch 5 Batch 1450 Loss 1.3559 Accuracy 0.4069\n",
      "Epoch 5 Batch 1500 Loss 1.3545 Accuracy 0.4070\n",
      "Epoch 5 Batch 1550 Loss 1.3532 Accuracy 0.4072\n",
      "Epoch 5 Batch 1600 Loss 1.3522 Accuracy 0.4074\n",
      "Epoch 5 Batch 1650 Loss 1.3501 Accuracy 0.4076\n",
      "Epoch 5 Batch 1700 Loss 1.3484 Accuracy 0.4080\n",
      "Epoch 5 Batch 1750 Loss 1.3469 Accuracy 0.4081\n",
      "Epoch 5 Batch 1800 Loss 1.3450 Accuracy 0.4083\n",
      "Epoch 5 Batch 1850 Loss 1.3423 Accuracy 0.4083\n",
      "Epoch 5 Batch 1900 Loss 1.3401 Accuracy 0.4083\n",
      "Epoch 5 Batch 1950 Loss 1.3384 Accuracy 0.4084\n",
      "Epoch 5 Batch 2000 Loss 1.3360 Accuracy 0.4085\n",
      "Epoch 5 Batch 2050 Loss 1.3338 Accuracy 0.4086\n",
      "Epoch 5 Batch 2100 Loss 1.3319 Accuracy 0.4086\n",
      "Epoch 5 Batch 2150 Loss 1.3295 Accuracy 0.4087\n",
      "Epoch 5 Batch 2200 Loss 1.3270 Accuracy 0.4087\n",
      "Epoch 5 Batch 2250 Loss 1.3246 Accuracy 0.4087\n",
      "Epoch 5 Batch 2300 Loss 1.3225 Accuracy 0.4087\n",
      "Epoch 5 Batch 2350 Loss 1.3207 Accuracy 0.4089\n",
      "Epoch 5 Batch 2400 Loss 1.3184 Accuracy 0.4090\n",
      "Epoch 5 Batch 2450 Loss 1.3156 Accuracy 0.4092\n",
      "Epoch 5 Batch 2500 Loss 1.3133 Accuracy 0.4094\n",
      "Epoch 5 Batch 2550 Loss 1.3110 Accuracy 0.4095\n",
      "Epoch 5 Batch 2600 Loss 1.3082 Accuracy 0.4097\n",
      "Epoch 5 Batch 2650 Loss 1.3059 Accuracy 0.4100\n",
      "Epoch 5 Batch 2700 Loss 1.3039 Accuracy 0.4104\n",
      "Epoch 5 Batch 2750 Loss 1.3017 Accuracy 0.4107\n",
      "Epoch 5 Batch 2800 Loss 1.2994 Accuracy 0.4111\n",
      "Epoch 5 Batch 2850 Loss 1.2972 Accuracy 0.4113\n",
      "Epoch 5 Batch 2900 Loss 1.2954 Accuracy 0.4116\n",
      "Epoch 5 Batch 2950 Loss 1.2939 Accuracy 0.4120\n",
      "Epoch 5 Batch 3000 Loss 1.2925 Accuracy 0.4125\n",
      "Epoch 5 Batch 3050 Loss 1.2913 Accuracy 0.4129\n",
      "Epoch 5 Batch 3100 Loss 1.2901 Accuracy 0.4132\n",
      "Epoch 5 Batch 3150 Loss 1.2882 Accuracy 0.4136\n",
      "Epoch 5 Batch 3200 Loss 1.2865 Accuracy 0.4140\n",
      "Epoch 5 Batch 3250 Loss 1.2849 Accuracy 0.4144\n",
      "Epoch 5 Batch 3300 Loss 1.2832 Accuracy 0.4149\n",
      "Epoch 5 Batch 3350 Loss 1.2819 Accuracy 0.4153\n",
      "Epoch 5 Batch 3400 Loss 1.2801 Accuracy 0.4157\n",
      "Epoch 5 Batch 3450 Loss 1.2789 Accuracy 0.4162\n",
      "Epoch 5 Batch 3500 Loss 1.2770 Accuracy 0.4166\n",
      "Epoch 5 Batch 3550 Loss 1.2751 Accuracy 0.4171\n",
      "Epoch 5 Batch 3600 Loss 1.2736 Accuracy 0.4175\n",
      "Epoch 5 Batch 3650 Loss 1.2715 Accuracy 0.4180\n",
      "Epoch 5 Batch 3700 Loss 1.2699 Accuracy 0.4184\n",
      "Epoch 5 Batch 3750 Loss 1.2683 Accuracy 0.4189\n",
      "Epoch 5 Batch 3800 Loss 1.2667 Accuracy 0.4194\n",
      "Epoch 5 Batch 3850 Loss 1.2653 Accuracy 0.4198\n",
      "Epoch 5 Batch 3900 Loss 1.2638 Accuracy 0.4203\n",
      "Epoch 5 Batch 3950 Loss 1.2622 Accuracy 0.4207\n",
      "Epoch 5 Batch 4000 Loss 1.2604 Accuracy 0.4212\n",
      "Epoch 5 Batch 4050 Loss 1.2588 Accuracy 0.4217\n",
      "Epoch 5 Batch 4100 Loss 1.2572 Accuracy 0.4222\n",
      "Epoch 5 Batch 4150 Loss 1.2556 Accuracy 0.4228\n",
      "Epoch 5 Batch 4200 Loss 1.2545 Accuracy 0.4233\n",
      "Epoch 5 Batch 4250 Loss 1.2528 Accuracy 0.4238\n",
      "Epoch 5 Batch 4300 Loss 1.2510 Accuracy 0.4242\n",
      "Epoch 5 Batch 4350 Loss 1.2498 Accuracy 0.4247\n",
      "Epoch 5 Batch 4400 Loss 1.2482 Accuracy 0.4252\n",
      "Epoch 5 Batch 4450 Loss 1.2472 Accuracy 0.4256\n",
      "Epoch 5 Batch 4500 Loss 1.2466 Accuracy 0.4259\n",
      "Epoch 5 Batch 4550 Loss 1.2468 Accuracy 0.4262\n",
      "Epoch 5 Batch 4600 Loss 1.2472 Accuracy 0.4263\n",
      "Epoch 5 Batch 4650 Loss 1.2477 Accuracy 0.4263\n",
      "Epoch 5 Batch 4700 Loss 1.2481 Accuracy 0.4264\n",
      "Epoch 5 Batch 4750 Loss 1.2487 Accuracy 0.4264\n",
      "Epoch 5 Batch 4800 Loss 1.2497 Accuracy 0.4263\n",
      "Epoch 5 Batch 4850 Loss 1.2504 Accuracy 0.4263\n",
      "Epoch 5 Batch 4900 Loss 1.2515 Accuracy 0.4262\n",
      "Epoch 5 Batch 4950 Loss 1.2523 Accuracy 0.4262\n",
      "Epoch 5 Batch 5000 Loss 1.2533 Accuracy 0.4261\n",
      "Epoch 5 Batch 5050 Loss 1.2544 Accuracy 0.4261\n",
      "Epoch 5 Batch 5100 Loss 1.2550 Accuracy 0.4260\n",
      "Epoch 5 Batch 5150 Loss 1.2556 Accuracy 0.4259\n",
      "Epoch 5 Batch 5200 Loss 1.2564 Accuracy 0.4259\n",
      "Epoch 5 Batch 5250 Loss 1.2574 Accuracy 0.4258\n",
      "Epoch 5 Batch 5300 Loss 1.2582 Accuracy 0.4258\n",
      "Epoch 5 Batch 5350 Loss 1.2587 Accuracy 0.4258\n",
      "Epoch 5 Batch 5400 Loss 1.2596 Accuracy 0.4257\n",
      "Epoch 5 Batch 5450 Loss 1.2604 Accuracy 0.4257\n",
      "Epoch 5 Batch 5500 Loss 1.2612 Accuracy 0.4256\n",
      "Epoch 5 Batch 5550 Loss 1.2619 Accuracy 0.4255\n",
      "Epoch 5 Batch 5600 Loss 1.2627 Accuracy 0.4254\n",
      "Epoch 5 Batch 5650 Loss 1.2635 Accuracy 0.4252\n",
      "Epoch 5 Batch 5700 Loss 1.2641 Accuracy 0.4251\n",
      "Epoch 5 Batch 5750 Loss 1.2644 Accuracy 0.4250\n",
      "Epoch 5 Batch 5800 Loss 1.2648 Accuracy 0.4248\n",
      "Epoch 5 Batch 5850 Loss 1.2655 Accuracy 0.4246\n",
      "Epoch 5 Batch 5900 Loss 1.2657 Accuracy 0.4245\n",
      "Epoch 5 Batch 5950 Loss 1.2662 Accuracy 0.4244\n",
      "Epoch 5 Batch 6000 Loss 1.2666 Accuracy 0.4243\n",
      "Epoch 5 Batch 6050 Loss 1.2671 Accuracy 0.4242\n",
      "Epoch 5 Batch 6100 Loss 1.2674 Accuracy 0.4240\n",
      "Epoch 5 Batch 6150 Loss 1.2675 Accuracy 0.4240\n",
      "Epoch 5 Batch 6200 Loss 1.2676 Accuracy 0.4238\n",
      "Saving checkpoint for epoch 5 at ./drive/My Drive/projects/transformer/ckpt/ckpt-5\n",
      "Time taken for 1 epoch: 1436.8817088603973 secs\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch 6 Batch 0 Loss 1.3930 Accuracy 0.4326\n",
      "Epoch 6 Batch 50 Loss 1.3648 Accuracy 0.4137\n",
      "Epoch 6 Batch 100 Loss 1.3593 Accuracy 0.4113\n",
      "Epoch 6 Batch 150 Loss 1.3518 Accuracy 0.4109\n",
      "Epoch 6 Batch 200 Loss 1.3478 Accuracy 0.4104\n",
      "Epoch 6 Batch 250 Loss 1.3411 Accuracy 0.4105\n",
      "Epoch 6 Batch 300 Loss 1.3375 Accuracy 0.4114\n",
      "Epoch 6 Batch 350 Loss 1.3353 Accuracy 0.4121\n",
      "Epoch 6 Batch 400 Loss 1.3354 Accuracy 0.4123\n",
      "Epoch 6 Batch 450 Loss 1.3333 Accuracy 0.4131\n",
      "Epoch 6 Batch 500 Loss 1.3319 Accuracy 0.4130\n",
      "Epoch 6 Batch 550 Loss 1.3284 Accuracy 0.4134\n",
      "Epoch 6 Batch 600 Loss 1.3277 Accuracy 0.4137\n",
      "Epoch 6 Batch 650 Loss 1.3286 Accuracy 0.4137\n",
      "Epoch 6 Batch 700 Loss 1.3285 Accuracy 0.4134\n",
      "Epoch 6 Batch 750 Loss 1.3264 Accuracy 0.4137\n",
      "Epoch 6 Batch 800 Loss 1.3247 Accuracy 0.4136\n",
      "Epoch 6 Batch 850 Loss 1.3241 Accuracy 0.4141\n",
      "Epoch 6 Batch 900 Loss 1.3243 Accuracy 0.4144\n",
      "Epoch 6 Batch 950 Loss 1.3247 Accuracy 0.4144\n",
      "Epoch 6 Batch 1000 Loss 1.3220 Accuracy 0.4145\n",
      "Epoch 6 Batch 1050 Loss 1.3207 Accuracy 0.4146\n",
      "Epoch 6 Batch 1100 Loss 1.3182 Accuracy 0.4148\n",
      "Epoch 6 Batch 1150 Loss 1.3171 Accuracy 0.4145\n",
      "Epoch 6 Batch 1200 Loss 1.3163 Accuracy 0.4145\n",
      "Epoch 6 Batch 1250 Loss 1.3149 Accuracy 0.4145\n",
      "Epoch 6 Batch 1300 Loss 1.3126 Accuracy 0.4144\n",
      "Epoch 6 Batch 1350 Loss 1.3105 Accuracy 0.4144\n",
      "Epoch 6 Batch 1400 Loss 1.3100 Accuracy 0.4144\n",
      "Epoch 6 Batch 1450 Loss 1.3085 Accuracy 0.4144\n",
      "Epoch 6 Batch 1500 Loss 1.3066 Accuracy 0.4147\n",
      "Epoch 6 Batch 1550 Loss 1.3057 Accuracy 0.4148\n",
      "Epoch 6 Batch 1600 Loss 1.3048 Accuracy 0.4151\n",
      "Epoch 6 Batch 1650 Loss 1.3020 Accuracy 0.4152\n",
      "Epoch 6 Batch 1700 Loss 1.3001 Accuracy 0.4153\n",
      "Epoch 6 Batch 1750 Loss 1.2976 Accuracy 0.4157\n",
      "Epoch 6 Batch 1800 Loss 1.2962 Accuracy 0.4157\n",
      "Epoch 6 Batch 1850 Loss 1.2946 Accuracy 0.4158\n",
      "Epoch 6 Batch 1900 Loss 1.2925 Accuracy 0.4158\n",
      "Epoch 6 Batch 1950 Loss 1.2899 Accuracy 0.4159\n",
      "Epoch 6 Batch 2000 Loss 1.2879 Accuracy 0.4159\n",
      "Epoch 6 Batch 2050 Loss 1.2857 Accuracy 0.4160\n",
      "Epoch 6 Batch 2100 Loss 1.2833 Accuracy 0.4159\n",
      "Epoch 6 Batch 2150 Loss 1.2816 Accuracy 0.4161\n",
      "Epoch 6 Batch 2200 Loss 1.2788 Accuracy 0.4161\n",
      "Epoch 6 Batch 2250 Loss 1.2761 Accuracy 0.4161\n",
      "Epoch 6 Batch 2300 Loss 1.2743 Accuracy 0.4162\n",
      "Epoch 6 Batch 2350 Loss 1.2728 Accuracy 0.4164\n",
      "Epoch 6 Batch 2400 Loss 1.2710 Accuracy 0.4165\n",
      "Epoch 6 Batch 2450 Loss 1.2683 Accuracy 0.4166\n",
      "Epoch 6 Batch 2500 Loss 1.2661 Accuracy 0.4166\n",
      "Epoch 6 Batch 2550 Loss 1.2637 Accuracy 0.4168\n",
      "Epoch 6 Batch 2600 Loss 1.2611 Accuracy 0.4170\n",
      "Epoch 6 Batch 2650 Loss 1.2587 Accuracy 0.4172\n",
      "Epoch 6 Batch 2700 Loss 1.2564 Accuracy 0.4175\n",
      "Epoch 6 Batch 2750 Loss 1.2542 Accuracy 0.4179\n",
      "Epoch 6 Batch 2800 Loss 1.2524 Accuracy 0.4183\n",
      "Epoch 6 Batch 2850 Loss 1.2508 Accuracy 0.4186\n",
      "Epoch 6 Batch 2900 Loss 1.2490 Accuracy 0.4189\n",
      "Epoch 6 Batch 2950 Loss 1.2476 Accuracy 0.4194\n",
      "Epoch 6 Batch 3000 Loss 1.2459 Accuracy 0.4196\n",
      "Epoch 6 Batch 3050 Loss 1.2447 Accuracy 0.4201\n",
      "Epoch 6 Batch 3100 Loss 1.2432 Accuracy 0.4205\n",
      "Epoch 6 Batch 3150 Loss 1.2414 Accuracy 0.4208\n",
      "Epoch 6 Batch 3200 Loss 1.2398 Accuracy 0.4212\n",
      "Epoch 6 Batch 3250 Loss 1.2388 Accuracy 0.4216\n",
      "Epoch 6 Batch 3300 Loss 1.2375 Accuracy 0.4221\n",
      "Epoch 6 Batch 3350 Loss 1.2365 Accuracy 0.4225\n",
      "Epoch 6 Batch 3400 Loss 1.2353 Accuracy 0.4229\n",
      "Epoch 6 Batch 3450 Loss 1.2340 Accuracy 0.4235\n",
      "Epoch 6 Batch 3500 Loss 1.2323 Accuracy 0.4238\n",
      "Epoch 6 Batch 3550 Loss 1.2307 Accuracy 0.4243\n",
      "Epoch 6 Batch 3600 Loss 1.2287 Accuracy 0.4247\n",
      "Epoch 6 Batch 3650 Loss 1.2272 Accuracy 0.4252\n",
      "Epoch 6 Batch 3700 Loss 1.2254 Accuracy 0.4256\n",
      "Epoch 6 Batch 3750 Loss 1.2237 Accuracy 0.4260\n",
      "Epoch 6 Batch 3800 Loss 1.2220 Accuracy 0.4264\n",
      "Epoch 6 Batch 3850 Loss 1.2206 Accuracy 0.4268\n",
      "Epoch 6 Batch 3900 Loss 1.2190 Accuracy 0.4274\n",
      "Epoch 6 Batch 3950 Loss 1.2174 Accuracy 0.4279\n",
      "Epoch 6 Batch 4000 Loss 1.2159 Accuracy 0.4284\n",
      "Epoch 6 Batch 4050 Loss 1.2144 Accuracy 0.4289\n",
      "Epoch 6 Batch 4100 Loss 1.2130 Accuracy 0.4294\n",
      "Epoch 6 Batch 4150 Loss 1.2115 Accuracy 0.4299\n",
      "Epoch 6 Batch 4200 Loss 1.2097 Accuracy 0.4303\n",
      "Epoch 6 Batch 4250 Loss 1.2077 Accuracy 0.4308\n",
      "Epoch 6 Batch 4300 Loss 1.2064 Accuracy 0.4313\n",
      "Epoch 6 Batch 4350 Loss 1.2050 Accuracy 0.4318\n",
      "Epoch 6 Batch 4400 Loss 1.2037 Accuracy 0.4323\n",
      "Epoch 6 Batch 4450 Loss 1.2026 Accuracy 0.4327\n",
      "Epoch 6 Batch 4500 Loss 1.2024 Accuracy 0.4330\n",
      "Epoch 6 Batch 4550 Loss 1.2023 Accuracy 0.4332\n",
      "Epoch 6 Batch 4600 Loss 1.2023 Accuracy 0.4332\n",
      "Epoch 6 Batch 4650 Loss 1.2028 Accuracy 0.4333\n",
      "Epoch 6 Batch 4700 Loss 1.2035 Accuracy 0.4333\n",
      "Epoch 6 Batch 4750 Loss 1.2043 Accuracy 0.4334\n",
      "Epoch 6 Batch 4800 Loss 1.2048 Accuracy 0.4333\n",
      "Epoch 6 Batch 4850 Loss 1.2059 Accuracy 0.4333\n",
      "Epoch 6 Batch 4900 Loss 1.2073 Accuracy 0.4332\n",
      "Epoch 6 Batch 4950 Loss 1.2082 Accuracy 0.4332\n",
      "Epoch 6 Batch 5000 Loss 1.2089 Accuracy 0.4331\n",
      "Epoch 6 Batch 5050 Loss 1.2099 Accuracy 0.4331\n",
      "Epoch 6 Batch 5100 Loss 1.2106 Accuracy 0.4330\n",
      "Epoch 6 Batch 5150 Loss 1.2113 Accuracy 0.4330\n",
      "Epoch 6 Batch 5200 Loss 1.2124 Accuracy 0.4329\n",
      "Epoch 6 Batch 5250 Loss 1.2132 Accuracy 0.4329\n",
      "Epoch 6 Batch 5300 Loss 1.2140 Accuracy 0.4328\n",
      "Epoch 6 Batch 5350 Loss 1.2148 Accuracy 0.4327\n",
      "Epoch 6 Batch 5400 Loss 1.2158 Accuracy 0.4326\n",
      "Epoch 6 Batch 5450 Loss 1.2168 Accuracy 0.4326\n",
      "Epoch 6 Batch 5500 Loss 1.2177 Accuracy 0.4325\n",
      "Epoch 6 Batch 5550 Loss 1.2185 Accuracy 0.4324\n",
      "Epoch 6 Batch 5600 Loss 1.2192 Accuracy 0.4323\n",
      "Epoch 6 Batch 5650 Loss 1.2198 Accuracy 0.4321\n",
      "Epoch 6 Batch 5700 Loss 1.2205 Accuracy 0.4320\n",
      "Epoch 6 Batch 5750 Loss 1.2211 Accuracy 0.4318\n",
      "Epoch 6 Batch 5800 Loss 1.2219 Accuracy 0.4316\n",
      "Epoch 6 Batch 5850 Loss 1.2223 Accuracy 0.4315\n",
      "Epoch 6 Batch 5900 Loss 1.2226 Accuracy 0.4313\n",
      "Epoch 6 Batch 5950 Loss 1.2230 Accuracy 0.4312\n",
      "Epoch 6 Batch 6000 Loss 1.2233 Accuracy 0.4310\n",
      "Epoch 6 Batch 6050 Loss 1.2239 Accuracy 0.4309\n",
      "Epoch 6 Batch 6100 Loss 1.2243 Accuracy 0.4308\n",
      "Epoch 6 Batch 6150 Loss 1.2244 Accuracy 0.4307\n",
      "Epoch 6 Batch 6200 Loss 1.2248 Accuracy 0.4306\n",
      "Saving checkpoint for epoch 6 at ./drive/My Drive/projects/transformer/ckpt/ckpt-6\n",
      "Time taken for 1 epoch: 1436.1207633018494 secs\n",
      "\n",
      "Start of epoch 7\n",
      "Epoch 7 Batch 0 Loss 1.4381 Accuracy 0.4400\n",
      "Epoch 7 Batch 50 Loss 1.3364 Accuracy 0.4184\n",
      "Epoch 7 Batch 100 Loss 1.3296 Accuracy 0.4162\n",
      "Epoch 7 Batch 150 Loss 1.3299 Accuracy 0.4161\n",
      "Epoch 7 Batch 200 Loss 1.3168 Accuracy 0.4176\n",
      "Epoch 7 Batch 250 Loss 1.3067 Accuracy 0.4185\n",
      "Epoch 7 Batch 300 Loss 1.3050 Accuracy 0.4179\n",
      "Epoch 7 Batch 350 Loss 1.3037 Accuracy 0.4176\n",
      "Epoch 7 Batch 400 Loss 1.3023 Accuracy 0.4176\n",
      "Epoch 7 Batch 450 Loss 1.2978 Accuracy 0.4183\n",
      "Epoch 7 Batch 500 Loss 1.2942 Accuracy 0.4184\n",
      "Epoch 7 Batch 550 Loss 1.2896 Accuracy 0.4188\n",
      "Epoch 7 Batch 600 Loss 1.2889 Accuracy 0.4190\n",
      "Epoch 7 Batch 650 Loss 1.2891 Accuracy 0.4190\n",
      "Epoch 7 Batch 700 Loss 1.2906 Accuracy 0.4191\n",
      "Epoch 7 Batch 750 Loss 1.2896 Accuracy 0.4191\n",
      "Epoch 7 Batch 800 Loss 1.2895 Accuracy 0.4190\n",
      "Epoch 7 Batch 850 Loss 1.2877 Accuracy 0.4192\n",
      "Epoch 7 Batch 900 Loss 1.2865 Accuracy 0.4193\n",
      "Epoch 7 Batch 950 Loss 1.2867 Accuracy 0.4194\n",
      "Epoch 7 Batch 1000 Loss 1.2864 Accuracy 0.4195\n",
      "Epoch 7 Batch 1050 Loss 1.2857 Accuracy 0.4195\n",
      "Epoch 7 Batch 1100 Loss 1.2839 Accuracy 0.4197\n",
      "Epoch 7 Batch 1150 Loss 1.2815 Accuracy 0.4198\n",
      "Epoch 7 Batch 1200 Loss 1.2798 Accuracy 0.4197\n",
      "Epoch 7 Batch 1250 Loss 1.2784 Accuracy 0.4199\n",
      "Epoch 7 Batch 1300 Loss 1.2775 Accuracy 0.4199\n",
      "Epoch 7 Batch 1350 Loss 1.2765 Accuracy 0.4197\n",
      "Epoch 7 Batch 1400 Loss 1.2754 Accuracy 0.4199\n",
      "Epoch 7 Batch 1450 Loss 1.2744 Accuracy 0.4199\n",
      "Epoch 7 Batch 1500 Loss 1.2731 Accuracy 0.4200\n",
      "Epoch 7 Batch 1550 Loss 1.2709 Accuracy 0.4202\n",
      "Epoch 7 Batch 1600 Loss 1.2688 Accuracy 0.4204\n",
      "Epoch 7 Batch 1650 Loss 1.2668 Accuracy 0.4205\n",
      "Epoch 7 Batch 1700 Loss 1.2643 Accuracy 0.4207\n",
      "Epoch 7 Batch 1750 Loss 1.2630 Accuracy 0.4209\n",
      "Epoch 7 Batch 1800 Loss 1.2600 Accuracy 0.4210\n",
      "Epoch 7 Batch 1850 Loss 1.2584 Accuracy 0.4212\n",
      "Epoch 7 Batch 1900 Loss 1.2563 Accuracy 0.4211\n",
      "Epoch 7 Batch 1950 Loss 1.2549 Accuracy 0.4212\n",
      "Epoch 7 Batch 2000 Loss 1.2527 Accuracy 0.4213\n",
      "Epoch 7 Batch 2050 Loss 1.2504 Accuracy 0.4215\n",
      "Epoch 7 Batch 2100 Loss 1.2478 Accuracy 0.4214\n",
      "Epoch 7 Batch 2150 Loss 1.2456 Accuracy 0.4215\n",
      "Epoch 7 Batch 2200 Loss 1.2434 Accuracy 0.4215\n",
      "Epoch 7 Batch 2250 Loss 1.2414 Accuracy 0.4215\n",
      "Epoch 7 Batch 2300 Loss 1.2400 Accuracy 0.4215\n",
      "Epoch 7 Batch 2350 Loss 1.2378 Accuracy 0.4217\n",
      "Epoch 7 Batch 2400 Loss 1.2354 Accuracy 0.4217\n",
      "Epoch 7 Batch 2450 Loss 1.2336 Accuracy 0.4218\n",
      "Epoch 7 Batch 2500 Loss 1.2307 Accuracy 0.4219\n",
      "Epoch 7 Batch 2550 Loss 1.2278 Accuracy 0.4221\n",
      "Epoch 7 Batch 2600 Loss 1.2253 Accuracy 0.4222\n",
      "Epoch 7 Batch 2650 Loss 1.2228 Accuracy 0.4225\n",
      "Epoch 7 Batch 2700 Loss 1.2207 Accuracy 0.4229\n",
      "Epoch 7 Batch 2750 Loss 1.2185 Accuracy 0.4232\n",
      "Epoch 7 Batch 2800 Loss 1.2166 Accuracy 0.4235\n",
      "Epoch 7 Batch 2850 Loss 1.2151 Accuracy 0.4239\n",
      "Epoch 7 Batch 2900 Loss 1.2135 Accuracy 0.4243\n",
      "Epoch 7 Batch 2950 Loss 1.2114 Accuracy 0.4247\n",
      "Epoch 7 Batch 3000 Loss 1.2098 Accuracy 0.4251\n",
      "Epoch 7 Batch 3050 Loss 1.2086 Accuracy 0.4256\n",
      "Epoch 7 Batch 3100 Loss 1.2076 Accuracy 0.4259\n",
      "Epoch 7 Batch 3150 Loss 1.2059 Accuracy 0.4263\n",
      "Epoch 7 Batch 3200 Loss 1.2043 Accuracy 0.4267\n",
      "Epoch 7 Batch 3250 Loss 1.2031 Accuracy 0.4270\n",
      "Epoch 7 Batch 3300 Loss 1.2019 Accuracy 0.4275\n",
      "Epoch 7 Batch 3350 Loss 1.2004 Accuracy 0.4279\n",
      "Epoch 7 Batch 3400 Loss 1.1993 Accuracy 0.4282\n",
      "Epoch 7 Batch 3450 Loss 1.1980 Accuracy 0.4286\n",
      "Epoch 7 Batch 3500 Loss 1.1965 Accuracy 0.4291\n",
      "Epoch 7 Batch 3550 Loss 1.1948 Accuracy 0.4295\n",
      "Epoch 7 Batch 3600 Loss 1.1933 Accuracy 0.4300\n",
      "Epoch 7 Batch 3650 Loss 1.1920 Accuracy 0.4304\n",
      "Epoch 7 Batch 3700 Loss 1.1903 Accuracy 0.4309\n",
      "Epoch 7 Batch 3750 Loss 1.1886 Accuracy 0.4314\n",
      "Epoch 7 Batch 3800 Loss 1.1870 Accuracy 0.4319\n",
      "Epoch 7 Batch 3850 Loss 1.1857 Accuracy 0.4323\n",
      "Epoch 7 Batch 3900 Loss 1.1839 Accuracy 0.4328\n",
      "Epoch 7 Batch 3950 Loss 1.1821 Accuracy 0.4333\n",
      "Epoch 7 Batch 4000 Loss 1.1805 Accuracy 0.4338\n",
      "Epoch 7 Batch 4050 Loss 1.1791 Accuracy 0.4343\n",
      "Epoch 7 Batch 4100 Loss 1.1778 Accuracy 0.4348\n",
      "Epoch 7 Batch 4150 Loss 1.1764 Accuracy 0.4352\n",
      "Epoch 7 Batch 4200 Loss 1.1750 Accuracy 0.4357\n",
      "Epoch 7 Batch 4250 Loss 1.1738 Accuracy 0.4361\n",
      "Epoch 7 Batch 4300 Loss 1.1722 Accuracy 0.4366\n",
      "Epoch 7 Batch 4350 Loss 1.1707 Accuracy 0.4370\n",
      "Epoch 7 Batch 4400 Loss 1.1694 Accuracy 0.4376\n",
      "Epoch 7 Batch 4450 Loss 1.1684 Accuracy 0.4379\n",
      "Epoch 7 Batch 4500 Loss 1.1679 Accuracy 0.4382\n",
      "Epoch 7 Batch 4550 Loss 1.1680 Accuracy 0.4385\n",
      "Epoch 7 Batch 4600 Loss 1.1684 Accuracy 0.4385\n",
      "Epoch 7 Batch 4650 Loss 1.1689 Accuracy 0.4385\n",
      "Epoch 7 Batch 4700 Loss 1.1695 Accuracy 0.4385\n",
      "Epoch 7 Batch 4750 Loss 1.1705 Accuracy 0.4385\n",
      "Epoch 7 Batch 4800 Loss 1.1712 Accuracy 0.4384\n",
      "Epoch 7 Batch 4850 Loss 1.1723 Accuracy 0.4384\n",
      "Epoch 7 Batch 4900 Loss 1.1729 Accuracy 0.4383\n",
      "Epoch 7 Batch 4950 Loss 1.1738 Accuracy 0.4382\n",
      "Epoch 7 Batch 5000 Loss 1.1747 Accuracy 0.4382\n",
      "Epoch 7 Batch 5050 Loss 1.1756 Accuracy 0.4381\n",
      "Epoch 7 Batch 5100 Loss 1.1765 Accuracy 0.4380\n",
      "Epoch 7 Batch 5150 Loss 1.1776 Accuracy 0.4380\n",
      "Epoch 7 Batch 5200 Loss 1.1785 Accuracy 0.4379\n",
      "Epoch 7 Batch 5250 Loss 1.1791 Accuracy 0.4378\n",
      "Epoch 7 Batch 5300 Loss 1.1800 Accuracy 0.4378\n",
      "Epoch 7 Batch 5350 Loss 1.1809 Accuracy 0.4377\n",
      "Epoch 7 Batch 5400 Loss 1.1817 Accuracy 0.4376\n",
      "Epoch 7 Batch 5450 Loss 1.1826 Accuracy 0.4376\n",
      "Epoch 7 Batch 5500 Loss 1.1837 Accuracy 0.4375\n",
      "Epoch 7 Batch 5550 Loss 1.1846 Accuracy 0.4374\n",
      "Epoch 7 Batch 5600 Loss 1.1854 Accuracy 0.4372\n",
      "Epoch 7 Batch 5650 Loss 1.1862 Accuracy 0.4371\n",
      "Epoch 7 Batch 5700 Loss 1.1870 Accuracy 0.4369\n",
      "Epoch 7 Batch 5750 Loss 1.1876 Accuracy 0.4367\n",
      "Epoch 7 Batch 5800 Loss 1.1883 Accuracy 0.4365\n",
      "Epoch 7 Batch 5850 Loss 1.1891 Accuracy 0.4364\n",
      "Epoch 7 Batch 5900 Loss 1.1897 Accuracy 0.4362\n",
      "Epoch 7 Batch 5950 Loss 1.1903 Accuracy 0.4361\n",
      "Epoch 7 Batch 6000 Loss 1.1906 Accuracy 0.4360\n",
      "Epoch 7 Batch 6050 Loss 1.1907 Accuracy 0.4358\n",
      "Epoch 7 Batch 6100 Loss 1.1912 Accuracy 0.4357\n",
      "Epoch 7 Batch 6150 Loss 1.1916 Accuracy 0.4356\n",
      "Epoch 7 Batch 6200 Loss 1.1918 Accuracy 0.4355\n",
      "Saving checkpoint for epoch 7 at ./drive/My Drive/projects/transformer/ckpt/ckpt-7\n",
      "Time taken for 1 epoch: 1437.1656420230865 secs\n",
      "\n",
      "Start of epoch 8\n",
      "Epoch 8 Batch 0 Loss 1.3906 Accuracy 0.4013\n",
      "Epoch 8 Batch 50 Loss 1.2898 Accuracy 0.4151\n",
      "Epoch 8 Batch 100 Loss 1.2834 Accuracy 0.4179\n",
      "Epoch 8 Batch 150 Loss 1.2820 Accuracy 0.4208\n",
      "Epoch 8 Batch 200 Loss 1.2805 Accuracy 0.4212\n",
      "Epoch 8 Batch 250 Loss 1.2776 Accuracy 0.4219\n",
      "Epoch 8 Batch 300 Loss 1.2732 Accuracy 0.4217\n",
      "Epoch 8 Batch 350 Loss 1.2720 Accuracy 0.4221\n",
      "Epoch 8 Batch 400 Loss 1.2676 Accuracy 0.4219\n",
      "Epoch 8 Batch 450 Loss 1.2693 Accuracy 0.4225\n",
      "Epoch 8 Batch 500 Loss 1.2666 Accuracy 0.4231\n",
      "Epoch 8 Batch 550 Loss 1.2649 Accuracy 0.4233\n",
      "Epoch 8 Batch 600 Loss 1.2648 Accuracy 0.4230\n",
      "Epoch 8 Batch 650 Loss 1.2613 Accuracy 0.4229\n",
      "Epoch 8 Batch 700 Loss 1.2603 Accuracy 0.4230\n",
      "Epoch 8 Batch 750 Loss 1.2611 Accuracy 0.4232\n",
      "Epoch 8 Batch 800 Loss 1.2599 Accuracy 0.4234\n",
      "Epoch 8 Batch 850 Loss 1.2589 Accuracy 0.4235\n",
      "Epoch 8 Batch 900 Loss 1.2578 Accuracy 0.4237\n",
      "Epoch 8 Batch 950 Loss 1.2580 Accuracy 0.4239\n",
      "Epoch 8 Batch 1000 Loss 1.2571 Accuracy 0.4242\n",
      "Epoch 8 Batch 1050 Loss 1.2561 Accuracy 0.4243\n",
      "Epoch 8 Batch 1100 Loss 1.2550 Accuracy 0.4243\n",
      "Epoch 8 Batch 1150 Loss 1.2536 Accuracy 0.4243\n",
      "Epoch 8 Batch 1200 Loss 1.2524 Accuracy 0.4244\n",
      "Epoch 8 Batch 1250 Loss 1.2506 Accuracy 0.4243\n",
      "Epoch 8 Batch 1300 Loss 1.2494 Accuracy 0.4244\n",
      "Epoch 8 Batch 1350 Loss 1.2479 Accuracy 0.4243\n",
      "Epoch 8 Batch 1400 Loss 1.2464 Accuracy 0.4245\n",
      "Epoch 8 Batch 1450 Loss 1.2445 Accuracy 0.4243\n",
      "Epoch 8 Batch 1500 Loss 1.2432 Accuracy 0.4245\n",
      "Epoch 8 Batch 1550 Loss 1.2419 Accuracy 0.4247\n",
      "Epoch 8 Batch 1600 Loss 1.2401 Accuracy 0.4249\n",
      "Epoch 8 Batch 1650 Loss 1.2381 Accuracy 0.4250\n",
      "Epoch 8 Batch 1700 Loss 1.2357 Accuracy 0.4252\n",
      "Epoch 8 Batch 1750 Loss 1.2332 Accuracy 0.4254\n",
      "Epoch 8 Batch 1800 Loss 1.2318 Accuracy 0.4256\n",
      "Epoch 8 Batch 1850 Loss 1.2299 Accuracy 0.4257\n",
      "Epoch 8 Batch 1900 Loss 1.2281 Accuracy 0.4258\n",
      "Epoch 8 Batch 1950 Loss 1.2261 Accuracy 0.4260\n",
      "Epoch 8 Batch 2000 Loss 1.2246 Accuracy 0.4259\n",
      "Epoch 8 Batch 2050 Loss 1.2224 Accuracy 0.4261\n",
      "Epoch 8 Batch 2100 Loss 1.2200 Accuracy 0.4259\n",
      "Epoch 8 Batch 2150 Loss 1.2175 Accuracy 0.4259\n",
      "Epoch 8 Batch 2200 Loss 1.2154 Accuracy 0.4260\n",
      "Epoch 8 Batch 2250 Loss 1.2135 Accuracy 0.4261\n",
      "Epoch 8 Batch 2300 Loss 1.2118 Accuracy 0.4262\n",
      "Epoch 8 Batch 2350 Loss 1.2095 Accuracy 0.4263\n",
      "Epoch 8 Batch 2400 Loss 1.2075 Accuracy 0.4263\n",
      "Epoch 8 Batch 2450 Loss 1.2052 Accuracy 0.4265\n",
      "Epoch 8 Batch 2500 Loss 1.2026 Accuracy 0.4267\n",
      "Epoch 8 Batch 2550 Loss 1.1999 Accuracy 0.4268\n",
      "Epoch 8 Batch 2600 Loss 1.1977 Accuracy 0.4270\n",
      "Epoch 8 Batch 2650 Loss 1.1951 Accuracy 0.4272\n",
      "Epoch 8 Batch 2700 Loss 1.1929 Accuracy 0.4275\n",
      "Epoch 8 Batch 2750 Loss 1.1913 Accuracy 0.4278\n",
      "Epoch 8 Batch 2800 Loss 1.1896 Accuracy 0.4282\n",
      "Epoch 8 Batch 2850 Loss 1.1880 Accuracy 0.4284\n",
      "Epoch 8 Batch 2900 Loss 1.1858 Accuracy 0.4288\n",
      "Epoch 8 Batch 2950 Loss 1.1842 Accuracy 0.4292\n",
      "Epoch 8 Batch 3000 Loss 1.1827 Accuracy 0.4296\n",
      "Epoch 8 Batch 3050 Loss 1.1813 Accuracy 0.4300\n",
      "Epoch 8 Batch 3100 Loss 1.1802 Accuracy 0.4303\n",
      "Epoch 8 Batch 3150 Loss 1.1791 Accuracy 0.4307\n",
      "Epoch 8 Batch 3200 Loss 1.1778 Accuracy 0.4311\n",
      "Epoch 8 Batch 3250 Loss 1.1762 Accuracy 0.4315\n",
      "Epoch 8 Batch 3300 Loss 1.1749 Accuracy 0.4319\n",
      "Epoch 8 Batch 3350 Loss 1.1737 Accuracy 0.4322\n",
      "Epoch 8 Batch 3400 Loss 1.1722 Accuracy 0.4326\n",
      "Epoch 8 Batch 3450 Loss 1.1705 Accuracy 0.4330\n",
      "Epoch 8 Batch 3500 Loss 1.1695 Accuracy 0.4334\n",
      "Epoch 8 Batch 3550 Loss 1.1683 Accuracy 0.4338\n",
      "Epoch 8 Batch 3600 Loss 1.1670 Accuracy 0.4343\n",
      "Epoch 8 Batch 3650 Loss 1.1653 Accuracy 0.4348\n",
      "Epoch 8 Batch 3700 Loss 1.1638 Accuracy 0.4352\n",
      "Epoch 8 Batch 3750 Loss 1.1622 Accuracy 0.4357\n",
      "Epoch 8 Batch 3800 Loss 1.1608 Accuracy 0.4361\n",
      "Epoch 8 Batch 3850 Loss 1.1592 Accuracy 0.4366\n",
      "Epoch 8 Batch 3900 Loss 1.1580 Accuracy 0.4370\n",
      "Epoch 8 Batch 3950 Loss 1.1562 Accuracy 0.4374\n",
      "Epoch 8 Batch 4000 Loss 1.1550 Accuracy 0.4379\n",
      "Epoch 8 Batch 4050 Loss 1.1533 Accuracy 0.4384\n",
      "Epoch 8 Batch 4100 Loss 1.1519 Accuracy 0.4389\n",
      "Epoch 8 Batch 4150 Loss 1.1503 Accuracy 0.4394\n",
      "Epoch 8 Batch 4200 Loss 1.1487 Accuracy 0.4399\n",
      "Epoch 8 Batch 4250 Loss 1.1471 Accuracy 0.4403\n",
      "Epoch 8 Batch 4300 Loss 1.1457 Accuracy 0.4408\n",
      "Epoch 8 Batch 4350 Loss 1.1445 Accuracy 0.4413\n",
      "Epoch 8 Batch 4400 Loss 1.1428 Accuracy 0.4417\n",
      "Epoch 8 Batch 4450 Loss 1.1415 Accuracy 0.4422\n",
      "Epoch 8 Batch 4500 Loss 1.1412 Accuracy 0.4425\n",
      "Epoch 8 Batch 4550 Loss 1.1410 Accuracy 0.4427\n",
      "Epoch 8 Batch 4600 Loss 1.1414 Accuracy 0.4429\n",
      "Epoch 8 Batch 4650 Loss 1.1419 Accuracy 0.4429\n",
      "Epoch 8 Batch 4700 Loss 1.1428 Accuracy 0.4429\n",
      "Epoch 8 Batch 4750 Loss 1.1435 Accuracy 0.4428\n",
      "Epoch 8 Batch 4800 Loss 1.1444 Accuracy 0.4428\n",
      "Epoch 8 Batch 4850 Loss 1.1451 Accuracy 0.4427\n",
      "Epoch 8 Batch 4900 Loss 1.1461 Accuracy 0.4426\n",
      "Epoch 8 Batch 4950 Loss 1.1473 Accuracy 0.4425\n",
      "Epoch 8 Batch 5000 Loss 1.1482 Accuracy 0.4425\n",
      "Epoch 8 Batch 5050 Loss 1.1494 Accuracy 0.4424\n",
      "Epoch 8 Batch 5100 Loss 1.1502 Accuracy 0.4424\n",
      "Epoch 8 Batch 5150 Loss 1.1512 Accuracy 0.4423\n",
      "Epoch 8 Batch 5200 Loss 1.1521 Accuracy 0.4423\n",
      "Epoch 8 Batch 5250 Loss 1.1530 Accuracy 0.4422\n",
      "Epoch 8 Batch 5300 Loss 1.1540 Accuracy 0.4422\n",
      "Epoch 8 Batch 5350 Loss 1.1552 Accuracy 0.4420\n",
      "Epoch 8 Batch 5400 Loss 1.1561 Accuracy 0.4420\n",
      "Epoch 8 Batch 5450 Loss 1.1568 Accuracy 0.4419\n",
      "Epoch 8 Batch 5500 Loss 1.1576 Accuracy 0.4418\n",
      "Epoch 8 Batch 5550 Loss 1.1584 Accuracy 0.4417\n",
      "Epoch 8 Batch 5600 Loss 1.1594 Accuracy 0.4415\n",
      "Epoch 8 Batch 5650 Loss 1.1601 Accuracy 0.4414\n",
      "Epoch 8 Batch 5700 Loss 1.1612 Accuracy 0.4412\n",
      "Epoch 8 Batch 5750 Loss 1.1618 Accuracy 0.4411\n",
      "Epoch 8 Batch 5800 Loss 1.1625 Accuracy 0.4409\n",
      "Epoch 8 Batch 5850 Loss 1.1631 Accuracy 0.4407\n",
      "Epoch 8 Batch 5900 Loss 1.1636 Accuracy 0.4405\n",
      "Epoch 8 Batch 5950 Loss 1.1642 Accuracy 0.4404\n",
      "Epoch 8 Batch 6000 Loss 1.1645 Accuracy 0.4402\n",
      "Epoch 8 Batch 6050 Loss 1.1650 Accuracy 0.4401\n",
      "Epoch 8 Batch 6100 Loss 1.1656 Accuracy 0.4399\n",
      "Epoch 8 Batch 6150 Loss 1.1659 Accuracy 0.4398\n",
      "Epoch 8 Batch 6200 Loss 1.1661 Accuracy 0.4397\n",
      "Saving checkpoint for epoch 8 at ./drive/My Drive/projects/transformer/ckpt/ckpt-8\n",
      "Time taken for 1 epoch: 1439.226893901825 secs\n",
      "\n",
      "Start of epoch 9\n",
      "Epoch 9 Batch 0 Loss 1.2197 Accuracy 0.4350\n",
      "Epoch 9 Batch 50 Loss 1.2667 Accuracy 0.4255\n",
      "Epoch 9 Batch 100 Loss 1.2628 Accuracy 0.4241\n",
      "Epoch 9 Batch 150 Loss 1.2669 Accuracy 0.4231\n",
      "Epoch 9 Batch 200 Loss 1.2665 Accuracy 0.4239\n",
      "Epoch 9 Batch 250 Loss 1.2618 Accuracy 0.4245\n",
      "Epoch 9 Batch 300 Loss 1.2564 Accuracy 0.4252\n",
      "Epoch 9 Batch 350 Loss 1.2515 Accuracy 0.4252\n",
      "Epoch 9 Batch 400 Loss 1.2491 Accuracy 0.4258\n",
      "Epoch 9 Batch 450 Loss 1.2447 Accuracy 0.4261\n",
      "Epoch 9 Batch 500 Loss 1.2435 Accuracy 0.4263\n",
      "Epoch 9 Batch 550 Loss 1.2426 Accuracy 0.4266\n",
      "Epoch 9 Batch 600 Loss 1.2392 Accuracy 0.4265\n",
      "Epoch 9 Batch 650 Loss 1.2378 Accuracy 0.4266\n",
      "Epoch 9 Batch 700 Loss 1.2377 Accuracy 0.4266\n",
      "Epoch 9 Batch 750 Loss 1.2375 Accuracy 0.4268\n",
      "Epoch 9 Batch 800 Loss 1.2377 Accuracy 0.4270\n",
      "Epoch 9 Batch 850 Loss 1.2360 Accuracy 0.4271\n",
      "Epoch 9 Batch 900 Loss 1.2349 Accuracy 0.4271\n",
      "Epoch 9 Batch 950 Loss 1.2349 Accuracy 0.4273\n",
      "Epoch 9 Batch 1000 Loss 1.2330 Accuracy 0.4277\n",
      "Epoch 9 Batch 1050 Loss 1.2313 Accuracy 0.4277\n",
      "Epoch 9 Batch 1100 Loss 1.2310 Accuracy 0.4278\n",
      "Epoch 9 Batch 1150 Loss 1.2292 Accuracy 0.4279\n",
      "Epoch 9 Batch 1200 Loss 1.2273 Accuracy 0.4280\n",
      "Epoch 9 Batch 1250 Loss 1.2268 Accuracy 0.4281\n",
      "Epoch 9 Batch 1300 Loss 1.2248 Accuracy 0.4280\n",
      "Epoch 9 Batch 1350 Loss 1.2228 Accuracy 0.4279\n",
      "Epoch 9 Batch 1400 Loss 1.2216 Accuracy 0.4280\n",
      "Epoch 9 Batch 1450 Loss 1.2200 Accuracy 0.4282\n",
      "Epoch 9 Batch 1500 Loss 1.2183 Accuracy 0.4282\n",
      "Epoch 9 Batch 1550 Loss 1.2167 Accuracy 0.4284\n",
      "Epoch 9 Batch 1600 Loss 1.2163 Accuracy 0.4285\n",
      "Epoch 9 Batch 1650 Loss 1.2153 Accuracy 0.4288\n",
      "Epoch 9 Batch 1700 Loss 1.2127 Accuracy 0.4291\n",
      "Epoch 9 Batch 1750 Loss 1.2103 Accuracy 0.4293\n",
      "Epoch 9 Batch 1800 Loss 1.2090 Accuracy 0.4294\n",
      "Epoch 9 Batch 1850 Loss 1.2069 Accuracy 0.4295\n",
      "Epoch 9 Batch 1900 Loss 1.2049 Accuracy 0.4295\n",
      "Epoch 9 Batch 1950 Loss 1.2022 Accuracy 0.4294\n",
      "Epoch 9 Batch 2000 Loss 1.1999 Accuracy 0.4295\n",
      "Epoch 9 Batch 2050 Loss 1.1982 Accuracy 0.4293\n",
      "Epoch 9 Batch 2100 Loss 1.1964 Accuracy 0.4294\n",
      "Epoch 9 Batch 2150 Loss 1.1941 Accuracy 0.4294\n",
      "Epoch 9 Batch 2200 Loss 1.1925 Accuracy 0.4295\n",
      "Epoch 9 Batch 2250 Loss 1.1902 Accuracy 0.4294\n",
      "Epoch 9 Batch 2300 Loss 1.1880 Accuracy 0.4295\n",
      "Epoch 9 Batch 2350 Loss 1.1867 Accuracy 0.4296\n",
      "Epoch 9 Batch 2400 Loss 1.1845 Accuracy 0.4297\n",
      "Epoch 9 Batch 2450 Loss 1.1821 Accuracy 0.4300\n",
      "Epoch 9 Batch 2500 Loss 1.1795 Accuracy 0.4300\n",
      "Epoch 9 Batch 2550 Loss 1.1771 Accuracy 0.4301\n",
      "Epoch 9 Batch 2600 Loss 1.1751 Accuracy 0.4302\n",
      "Epoch 9 Batch 2650 Loss 1.1727 Accuracy 0.4305\n",
      "Epoch 9 Batch 2700 Loss 1.1706 Accuracy 0.4309\n",
      "Epoch 9 Batch 2750 Loss 1.1691 Accuracy 0.4312\n",
      "Epoch 9 Batch 2800 Loss 1.1669 Accuracy 0.4316\n",
      "Epoch 9 Batch 2850 Loss 1.1653 Accuracy 0.4320\n",
      "Epoch 9 Batch 2900 Loss 1.1637 Accuracy 0.4323\n",
      "Epoch 9 Batch 2950 Loss 1.1622 Accuracy 0.4327\n",
      "Epoch 9 Batch 3000 Loss 1.1604 Accuracy 0.4330\n",
      "Epoch 9 Batch 3050 Loss 1.1588 Accuracy 0.4334\n",
      "Epoch 9 Batch 3100 Loss 1.1574 Accuracy 0.4338\n",
      "Epoch 9 Batch 3150 Loss 1.1561 Accuracy 0.4342\n",
      "Epoch 9 Batch 3200 Loss 1.1544 Accuracy 0.4345\n",
      "Epoch 9 Batch 3250 Loss 1.1529 Accuracy 0.4349\n",
      "Epoch 9 Batch 3300 Loss 1.1518 Accuracy 0.4353\n",
      "Epoch 9 Batch 3350 Loss 1.1508 Accuracy 0.4357\n",
      "Epoch 9 Batch 3400 Loss 1.1500 Accuracy 0.4361\n",
      "Epoch 9 Batch 3450 Loss 1.1482 Accuracy 0.4365\n",
      "Epoch 9 Batch 3500 Loss 1.1466 Accuracy 0.4369\n",
      "Epoch 9 Batch 3550 Loss 1.1451 Accuracy 0.4374\n",
      "Epoch 9 Batch 3600 Loss 1.1439 Accuracy 0.4378\n",
      "Epoch 9 Batch 3650 Loss 1.1423 Accuracy 0.4383\n",
      "Epoch 9 Batch 3700 Loss 1.1406 Accuracy 0.4387\n",
      "Epoch 9 Batch 3750 Loss 1.1389 Accuracy 0.4391\n",
      "Epoch 9 Batch 3800 Loss 1.1371 Accuracy 0.4396\n",
      "Epoch 9 Batch 3850 Loss 1.1358 Accuracy 0.4400\n",
      "Epoch 9 Batch 3900 Loss 1.1342 Accuracy 0.4406\n",
      "Epoch 9 Batch 3950 Loss 1.1325 Accuracy 0.4410\n",
      "Epoch 9 Batch 4000 Loss 1.1311 Accuracy 0.4415\n",
      "Epoch 9 Batch 4050 Loss 1.1298 Accuracy 0.4420\n",
      "Epoch 9 Batch 4100 Loss 1.1285 Accuracy 0.4424\n",
      "Epoch 9 Batch 4150 Loss 1.1272 Accuracy 0.4429\n",
      "Epoch 9 Batch 4200 Loss 1.1260 Accuracy 0.4433\n",
      "Epoch 9 Batch 4250 Loss 1.1248 Accuracy 0.4438\n",
      "Epoch 9 Batch 4300 Loss 1.1233 Accuracy 0.4442\n",
      "Epoch 9 Batch 4350 Loss 1.1221 Accuracy 0.4447\n",
      "Epoch 9 Batch 4400 Loss 1.1206 Accuracy 0.4452\n",
      "Epoch 9 Batch 4450 Loss 1.1196 Accuracy 0.4456\n",
      "Epoch 9 Batch 4500 Loss 1.1191 Accuracy 0.4459\n",
      "Epoch 9 Batch 4550 Loss 1.1194 Accuracy 0.4461\n",
      "Epoch 9 Batch 4600 Loss 1.1195 Accuracy 0.4462\n",
      "Epoch 9 Batch 4650 Loss 1.1200 Accuracy 0.4463\n",
      "Epoch 9 Batch 4700 Loss 1.1206 Accuracy 0.4463\n",
      "Epoch 9 Batch 4750 Loss 1.1213 Accuracy 0.4462\n",
      "Epoch 9 Batch 4800 Loss 1.1220 Accuracy 0.4461\n",
      "Epoch 9 Batch 4850 Loss 1.1231 Accuracy 0.4460\n",
      "Epoch 9 Batch 4900 Loss 1.1241 Accuracy 0.4460\n",
      "Epoch 9 Batch 4950 Loss 1.1252 Accuracy 0.4459\n",
      "Epoch 9 Batch 5000 Loss 1.1262 Accuracy 0.4458\n",
      "Epoch 9 Batch 5050 Loss 1.1272 Accuracy 0.4457\n",
      "Epoch 9 Batch 5100 Loss 1.1282 Accuracy 0.4456\n",
      "Epoch 9 Batch 5150 Loss 1.1292 Accuracy 0.4455\n",
      "Epoch 9 Batch 5200 Loss 1.1304 Accuracy 0.4455\n",
      "Epoch 9 Batch 5250 Loss 1.1313 Accuracy 0.4454\n",
      "Epoch 9 Batch 5300 Loss 1.1322 Accuracy 0.4454\n",
      "Epoch 9 Batch 5350 Loss 1.1329 Accuracy 0.4453\n",
      "Epoch 9 Batch 5400 Loss 1.1338 Accuracy 0.4452\n",
      "Epoch 9 Batch 5450 Loss 1.1348 Accuracy 0.4451\n",
      "Epoch 9 Batch 5500 Loss 1.1357 Accuracy 0.4450\n",
      "Epoch 9 Batch 5550 Loss 1.1365 Accuracy 0.4449\n",
      "Epoch 9 Batch 5600 Loss 1.1375 Accuracy 0.4447\n",
      "Epoch 9 Batch 5650 Loss 1.1383 Accuracy 0.4446\n",
      "Epoch 9 Batch 5700 Loss 1.1394 Accuracy 0.4445\n",
      "Epoch 9 Batch 5750 Loss 1.1403 Accuracy 0.4443\n",
      "Epoch 9 Batch 5800 Loss 1.1409 Accuracy 0.4441\n",
      "Epoch 9 Batch 5850 Loss 1.1414 Accuracy 0.4439\n",
      "Epoch 9 Batch 5900 Loss 1.1420 Accuracy 0.4437\n",
      "Epoch 9 Batch 5950 Loss 1.1423 Accuracy 0.4436\n",
      "Epoch 9 Batch 6000 Loss 1.1427 Accuracy 0.4435\n",
      "Epoch 9 Batch 6050 Loss 1.1430 Accuracy 0.4433\n",
      "Epoch 9 Batch 6100 Loss 1.1435 Accuracy 0.4432\n",
      "Epoch 9 Batch 6150 Loss 1.1440 Accuracy 0.4431\n",
      "Epoch 9 Batch 6200 Loss 1.1446 Accuracy 0.4429\n",
      "Saving checkpoint for epoch 9 at ./drive/My Drive/projects/transformer/ckpt/ckpt-9\n",
      "Time taken for 1 epoch: 1437.3798534870148 secs\n",
      "\n",
      "Start of epoch 10\n",
      "Epoch 10 Batch 0 Loss 1.2841 Accuracy 0.4416\n",
      "Epoch 10 Batch 50 Loss 1.2347 Accuracy 0.4296\n",
      "Epoch 10 Batch 100 Loss 1.2385 Accuracy 0.4288\n",
      "Epoch 10 Batch 150 Loss 1.2388 Accuracy 0.4292\n",
      "Epoch 10 Batch 200 Loss 1.2359 Accuracy 0.4299\n",
      "Epoch 10 Batch 250 Loss 1.2396 Accuracy 0.4307\n",
      "Epoch 10 Batch 300 Loss 1.2379 Accuracy 0.4307\n",
      "Epoch 10 Batch 350 Loss 1.2381 Accuracy 0.4305\n",
      "Epoch 10 Batch 400 Loss 1.2366 Accuracy 0.4301\n",
      "Epoch 10 Batch 450 Loss 1.2324 Accuracy 0.4302\n",
      "Epoch 10 Batch 500 Loss 1.2272 Accuracy 0.4305\n",
      "Epoch 10 Batch 550 Loss 1.2249 Accuracy 0.4308\n",
      "Epoch 10 Batch 600 Loss 1.2220 Accuracy 0.4304\n",
      "Epoch 10 Batch 650 Loss 1.2204 Accuracy 0.4303\n",
      "Epoch 10 Batch 700 Loss 1.2197 Accuracy 0.4304\n",
      "Epoch 10 Batch 750 Loss 1.2198 Accuracy 0.4305\n",
      "Epoch 10 Batch 800 Loss 1.2184 Accuracy 0.4305\n",
      "Epoch 10 Batch 850 Loss 1.2188 Accuracy 0.4305\n",
      "Epoch 10 Batch 900 Loss 1.2180 Accuracy 0.4305\n",
      "Epoch 10 Batch 950 Loss 1.2158 Accuracy 0.4306\n",
      "Epoch 10 Batch 1000 Loss 1.2156 Accuracy 0.4306\n",
      "Epoch 10 Batch 1050 Loss 1.2143 Accuracy 0.4306\n",
      "Epoch 10 Batch 1100 Loss 1.2126 Accuracy 0.4309\n",
      "Epoch 10 Batch 1150 Loss 1.2105 Accuracy 0.4309\n",
      "Epoch 10 Batch 1200 Loss 1.2086 Accuracy 0.4310\n",
      "Epoch 10 Batch 1250 Loss 1.2073 Accuracy 0.4310\n",
      "Epoch 10 Batch 1300 Loss 1.2059 Accuracy 0.4309\n",
      "Epoch 10 Batch 1350 Loss 1.2038 Accuracy 0.4309\n",
      "Epoch 10 Batch 1400 Loss 1.2026 Accuracy 0.4308\n",
      "Epoch 10 Batch 1450 Loss 1.2016 Accuracy 0.4310\n",
      "Epoch 10 Batch 1500 Loss 1.2003 Accuracy 0.4312\n",
      "Epoch 10 Batch 1550 Loss 1.2001 Accuracy 0.4312\n",
      "Epoch 10 Batch 1600 Loss 1.1987 Accuracy 0.4315\n",
      "Epoch 10 Batch 1650 Loss 1.1967 Accuracy 0.4316\n",
      "Epoch 10 Batch 1700 Loss 1.1947 Accuracy 0.4317\n",
      "Epoch 10 Batch 1750 Loss 1.1930 Accuracy 0.4319\n",
      "Epoch 10 Batch 1800 Loss 1.1907 Accuracy 0.4320\n",
      "Epoch 10 Batch 1850 Loss 1.1887 Accuracy 0.4320\n",
      "Epoch 10 Batch 1900 Loss 1.1867 Accuracy 0.4320\n",
      "Epoch 10 Batch 1950 Loss 1.1844 Accuracy 0.4321\n",
      "Epoch 10 Batch 2000 Loss 1.1825 Accuracy 0.4321\n",
      "Epoch 10 Batch 2050 Loss 1.1801 Accuracy 0.4321\n",
      "Epoch 10 Batch 2100 Loss 1.1780 Accuracy 0.4320\n",
      "Epoch 10 Batch 2150 Loss 1.1770 Accuracy 0.4323\n",
      "Epoch 10 Batch 2200 Loss 1.1750 Accuracy 0.4323\n",
      "Epoch 10 Batch 2250 Loss 1.1730 Accuracy 0.4323\n",
      "Epoch 10 Batch 2300 Loss 1.1712 Accuracy 0.4324\n",
      "Epoch 10 Batch 2350 Loss 1.1689 Accuracy 0.4324\n",
      "Epoch 10 Batch 2400 Loss 1.1668 Accuracy 0.4326\n",
      "Epoch 10 Batch 2450 Loss 1.1647 Accuracy 0.4327\n",
      "Epoch 10 Batch 2500 Loss 1.1618 Accuracy 0.4328\n",
      "Epoch 10 Batch 2550 Loss 1.1590 Accuracy 0.4330\n",
      "Epoch 10 Batch 2600 Loss 1.1564 Accuracy 0.4332\n",
      "Epoch 10 Batch 2650 Loss 1.1543 Accuracy 0.4334\n",
      "Epoch 10 Batch 2700 Loss 1.1523 Accuracy 0.4337\n",
      "Epoch 10 Batch 2750 Loss 1.1506 Accuracy 0.4341\n",
      "Epoch 10 Batch 2800 Loss 1.1491 Accuracy 0.4344\n",
      "Epoch 10 Batch 2850 Loss 1.1471 Accuracy 0.4347\n",
      "Epoch 10 Batch 2900 Loss 1.1453 Accuracy 0.4350\n",
      "Epoch 10 Batch 2950 Loss 1.1435 Accuracy 0.4354\n",
      "Epoch 10 Batch 3000 Loss 1.1416 Accuracy 0.4358\n",
      "Epoch 10 Batch 3050 Loss 1.1405 Accuracy 0.4361\n",
      "Epoch 10 Batch 3100 Loss 1.1393 Accuracy 0.4365\n",
      "Epoch 10 Batch 3150 Loss 1.1376 Accuracy 0.4369\n",
      "Epoch 10 Batch 3200 Loss 1.1365 Accuracy 0.4373\n",
      "Epoch 10 Batch 3250 Loss 1.1354 Accuracy 0.4376\n",
      "Epoch 10 Batch 3300 Loss 1.1340 Accuracy 0.4380\n",
      "Epoch 10 Batch 3350 Loss 1.1327 Accuracy 0.4383\n",
      "Epoch 10 Batch 3400 Loss 1.1314 Accuracy 0.4387\n",
      "Epoch 10 Batch 3450 Loss 1.1301 Accuracy 0.4391\n",
      "Epoch 10 Batch 3500 Loss 1.1283 Accuracy 0.4396\n",
      "Epoch 10 Batch 3550 Loss 1.1270 Accuracy 0.4401\n",
      "Epoch 10 Batch 3600 Loss 1.1255 Accuracy 0.4406\n",
      "Epoch 10 Batch 3650 Loss 1.1240 Accuracy 0.4410\n",
      "Epoch 10 Batch 3700 Loss 1.1224 Accuracy 0.4414\n",
      "Epoch 10 Batch 3750 Loss 1.1210 Accuracy 0.4418\n",
      "Epoch 10 Batch 3800 Loss 1.1197 Accuracy 0.4422\n",
      "Epoch 10 Batch 3850 Loss 1.1183 Accuracy 0.4427\n",
      "Epoch 10 Batch 3900 Loss 1.1166 Accuracy 0.4431\n",
      "Epoch 10 Batch 3950 Loss 1.1151 Accuracy 0.4436\n",
      "Epoch 10 Batch 4000 Loss 1.1139 Accuracy 0.4441\n",
      "Epoch 10 Batch 4050 Loss 1.1123 Accuracy 0.4446\n",
      "Epoch 10 Batch 4100 Loss 1.1110 Accuracy 0.4451\n",
      "Epoch 10 Batch 4150 Loss 1.1094 Accuracy 0.4456\n",
      "Epoch 10 Batch 4200 Loss 1.1078 Accuracy 0.4461\n",
      "Epoch 10 Batch 4250 Loss 1.1063 Accuracy 0.4466\n",
      "Epoch 10 Batch 4300 Loss 1.1051 Accuracy 0.4471\n",
      "Epoch 10 Batch 4350 Loss 1.1037 Accuracy 0.4476\n",
      "Epoch 10 Batch 4400 Loss 1.1025 Accuracy 0.4481\n",
      "Epoch 10 Batch 4450 Loss 1.1015 Accuracy 0.4485\n",
      "Epoch 10 Batch 4500 Loss 1.1009 Accuracy 0.4487\n",
      "Epoch 10 Batch 4550 Loss 1.1013 Accuracy 0.4489\n",
      "Epoch 10 Batch 4600 Loss 1.1015 Accuracy 0.4490\n",
      "Epoch 10 Batch 4650 Loss 1.1022 Accuracy 0.4490\n",
      "Epoch 10 Batch 4700 Loss 1.1029 Accuracy 0.4490\n",
      "Epoch 10 Batch 4750 Loss 1.1037 Accuracy 0.4490\n",
      "Epoch 10 Batch 4800 Loss 1.1048 Accuracy 0.4489\n",
      "Epoch 10 Batch 4850 Loss 1.1055 Accuracy 0.4488\n",
      "Epoch 10 Batch 4900 Loss 1.1068 Accuracy 0.4487\n",
      "Epoch 10 Batch 4950 Loss 1.1079 Accuracy 0.4487\n",
      "Epoch 10 Batch 5000 Loss 1.1088 Accuracy 0.4486\n",
      "Epoch 10 Batch 5050 Loss 1.1097 Accuracy 0.4486\n",
      "Epoch 10 Batch 5100 Loss 1.1107 Accuracy 0.4484\n",
      "Epoch 10 Batch 5150 Loss 1.1114 Accuracy 0.4484\n",
      "Epoch 10 Batch 5200 Loss 1.1124 Accuracy 0.4483\n",
      "Epoch 10 Batch 5250 Loss 1.1132 Accuracy 0.4482\n",
      "Epoch 10 Batch 5300 Loss 1.1142 Accuracy 0.4481\n",
      "Epoch 10 Batch 5350 Loss 1.1148 Accuracy 0.4481\n",
      "Epoch 10 Batch 5400 Loss 1.1158 Accuracy 0.4480\n",
      "Epoch 10 Batch 5450 Loss 1.1166 Accuracy 0.4478\n",
      "Epoch 10 Batch 5500 Loss 1.1176 Accuracy 0.4478\n",
      "Epoch 10 Batch 5550 Loss 1.1186 Accuracy 0.4477\n",
      "Epoch 10 Batch 5600 Loss 1.1193 Accuracy 0.4475\n",
      "Epoch 10 Batch 5650 Loss 1.1203 Accuracy 0.4473\n",
      "Epoch 10 Batch 5700 Loss 1.1209 Accuracy 0.4471\n",
      "Epoch 10 Batch 5750 Loss 1.1221 Accuracy 0.4470\n",
      "Epoch 10 Batch 5800 Loss 1.1227 Accuracy 0.4467\n",
      "Epoch 10 Batch 5850 Loss 1.1235 Accuracy 0.4466\n",
      "Epoch 10 Batch 5900 Loss 1.1242 Accuracy 0.4465\n",
      "Epoch 10 Batch 5950 Loss 1.1248 Accuracy 0.4463\n",
      "Epoch 10 Batch 6000 Loss 1.1251 Accuracy 0.4461\n",
      "Epoch 10 Batch 6050 Loss 1.1254 Accuracy 0.4460\n",
      "Epoch 10 Batch 6100 Loss 1.1257 Accuracy 0.4459\n",
      "Epoch 10 Batch 6150 Loss 1.1261 Accuracy 0.4458\n",
      "Epoch 10 Batch 6200 Loss 1.1266 Accuracy 0.4456\n",
      "Saving checkpoint for epoch 10 at ./drive/My Drive/projects/transformer/ckpt/ckpt-10\n",
      "Time taken for 1 epoch: 1437.3767762184143 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Start of epoch {}\".format(epoch+1))\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
    "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
    "                                                        ckpt_save_path))\n",
    "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmzyRwDrRGdq"
   },
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "QV1_KoyOZsEx"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = \\\n",
    "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims([VOCAB_SIZE_DE-2], axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = transformer(enc_input, output, False)\n",
    "        \n",
    "        prediction = predictions[:, -1:, :]\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == VOCAB_SIZE_DE-1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ox4x0TcwZsx-"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = tokenizer_de.decode(\n",
    "        [i for i in output if i < VOCAB_SIZE_DE-2]\n",
    "    )\n",
    "    \n",
    "    print(\"Input: {}\".format(sentence))\n",
    "    print(\"Predicted translation: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhVKBjR1ZvSF",
    "outputId": "d360335c-174a-4862-fd75-ba7aee9996c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I am from Germany\n",
      "Predicted translation: Ich bin aus Deutschland.\n"
     ]
    }
   ],
   "source": [
    "translate(\"I am from Germany\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of Transformer_for_NLP_udemy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
